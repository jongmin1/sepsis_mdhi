{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 581,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Gqlpt6bO5ZzD",
    "outputId": "04886b08-66a6-4076-c8a9-7d78da19622b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.9.0+cu111\n",
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/IPython/core/magics/pylab.py:160: UserWarning: pylab import has clobbered these variables: ['test']\n",
      "`%matplotlib` prevents importing * from pylab and numpy\n",
      "  \"\\n`%matplotlib` prevents importing * from pylab and numpy\"\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import datetime\n",
    "import torch\n",
    "from torch.autograd import Variable\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import argparse\n",
    "from copy import deepcopy # Add Deepcopy for args\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.metrics import roc_auc_score\n",
    "import seaborn as sns \n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.preprocessing import binarize\n",
    "\n",
    "\n",
    "print(torch.__version__)\n",
    "%matplotlib inline\n",
    "%pylab inline\n",
    "pylab.rcParams['figure.figsize'] = (15, 9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "__DRQb_p5ZzE"
   },
   "source": [
    "# Pandas Datareader Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 582,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4804\n"
     ]
    }
   ],
   "source": [
    "total = pd.read_csv('./result/total.csv')\n",
    "print(len(total))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 583,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4804\n",
      "4804\n"
     ]
    }
   ],
   "source": [
    "print(len(total))\n",
    "temp = total[total['is_infection']==1]\n",
    "# total = pd.concat([total, temp])\n",
    "# total = pd.concat([total, temp])\n",
    "# total = pd.concat([total, temp])\n",
    "# total = pd.concat([total, temp])\n",
    "# total = pd.concat([total, temp])\n",
    "# total = pd.concat([total, temp])\n",
    "# total = pd.concat([total, temp])\n",
    "# total = pd.concat([total, temp])\n",
    "print(len(total))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 584,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.series.Series'>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Index(['Unnamed: 0', 'subject_id', 'stay_id', 'compiled_hr', 'endtime',\n",
       "       'pao2ratio_vent', 'rate_dopamine', 'rate_epinephrine',\n",
       "       'rate_nonepinephrine', 'meabp_min', 'heartrate_max', 'temperature_max',\n",
       "       'gcs_min', 'bilirubin_max', 'creatineine_max', 'paltelet_min',\n",
       "       'respiration', 'coagulation', 'liver', 'cns', 'renal',\n",
       "       'respiration_24hours', 'coagulation_24hours', 'liver_24hours',\n",
       "       'cns_24hours', 'renal_24hours', 'sofa_24hours', 'infection_time',\n",
       "       'is_infection', 'infection_hour'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 584,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total.columns\n",
    "# print(type(total))\n",
    "print(type(total['Unnamed: 0']))\n",
    "total['Unnamed: 0'] = pd.to_datetime(total['Unnamed: 0'])\n",
    "total.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 585,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>subject_id</th>\n",
       "      <th>stay_id</th>\n",
       "      <th>compiled_hr</th>\n",
       "      <th>endtime</th>\n",
       "      <th>pao2ratio_vent</th>\n",
       "      <th>rate_dopamine</th>\n",
       "      <th>rate_epinephrine</th>\n",
       "      <th>rate_nonepinephrine</th>\n",
       "      <th>meabp_min</th>\n",
       "      <th>...</th>\n",
       "      <th>renal</th>\n",
       "      <th>respiration_24hours</th>\n",
       "      <th>coagulation_24hours</th>\n",
       "      <th>liver_24hours</th>\n",
       "      <th>cns_24hours</th>\n",
       "      <th>renal_24hours</th>\n",
       "      <th>sofa_24hours</th>\n",
       "      <th>infection_time</th>\n",
       "      <th>is_infection</th>\n",
       "      <th>infection_hour</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2112-02-16 19:00:00</td>\n",
       "      <td>14034916</td>\n",
       "      <td>39521325</td>\n",
       "      <td>0</td>\n",
       "      <td>2112-02-17 03:00:00</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2112-02-17 03:00:00</td>\n",
       "      <td>14034916</td>\n",
       "      <td>39521325</td>\n",
       "      <td>1</td>\n",
       "      <td>2112-02-17 11:00:00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.147887</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2112-02-17 11:00:00</td>\n",
       "      <td>14034916</td>\n",
       "      <td>39521325</td>\n",
       "      <td>2</td>\n",
       "      <td>2112-02-17 19:00:00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.866197</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2112-02-17 19:00:00</td>\n",
       "      <td>14034916</td>\n",
       "      <td>39521325</td>\n",
       "      <td>3</td>\n",
       "      <td>2112-02-18 03:00:00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.133803</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2131-10-11 14:00:00</td>\n",
       "      <td>12588462</td>\n",
       "      <td>32189053</td>\n",
       "      <td>0</td>\n",
       "      <td>2131-10-11 22:00:00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.008621</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2131-10-11 22:00:00</td>\n",
       "      <td>12588462</td>\n",
       "      <td>32189053</td>\n",
       "      <td>1</td>\n",
       "      <td>2131-10-12 06:00:00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.212644</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>2131-10-12 06:00:00</td>\n",
       "      <td>12588462</td>\n",
       "      <td>32189053</td>\n",
       "      <td>2</td>\n",
       "      <td>2131-10-12 14:00:00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.540230</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>2131-10-12 14:00:00</td>\n",
       "      <td>12588462</td>\n",
       "      <td>32189053</td>\n",
       "      <td>3</td>\n",
       "      <td>2131-10-12 22:00:00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.413793</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.75</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>2128-10-15 03:00:00</td>\n",
       "      <td>17188632</td>\n",
       "      <td>31948656</td>\n",
       "      <td>0</td>\n",
       "      <td>2128-10-15 11:00:00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>2128-10-15 11:00:00</td>\n",
       "      <td>17188632</td>\n",
       "      <td>31948656</td>\n",
       "      <td>1</td>\n",
       "      <td>2128-10-15 19:00:00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.429630</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10 rows × 30 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           Unnamed: 0  subject_id   stay_id  compiled_hr              endtime  \\\n",
       "0 2112-02-16 19:00:00    14034916  39521325            0  2112-02-17 03:00:00   \n",
       "1 2112-02-17 03:00:00    14034916  39521325            1  2112-02-17 11:00:00   \n",
       "2 2112-02-17 11:00:00    14034916  39521325            2  2112-02-17 19:00:00   \n",
       "3 2112-02-17 19:00:00    14034916  39521325            3  2112-02-18 03:00:00   \n",
       "4 2131-10-11 14:00:00    12588462  32189053            0  2131-10-11 22:00:00   \n",
       "5 2131-10-11 22:00:00    12588462  32189053            1  2131-10-12 06:00:00   \n",
       "6 2131-10-12 06:00:00    12588462  32189053            2  2131-10-12 14:00:00   \n",
       "7 2131-10-12 14:00:00    12588462  32189053            3  2131-10-12 22:00:00   \n",
       "8 2128-10-15 03:00:00    17188632  31948656            0  2128-10-15 11:00:00   \n",
       "9 2128-10-15 11:00:00    17188632  31948656            1  2128-10-15 19:00:00   \n",
       "\n",
       "   pao2ratio_vent  rate_dopamine  rate_epinephrine  rate_nonepinephrine  \\\n",
       "0             1.0            0.0               0.0                  0.0   \n",
       "1             0.0            0.0               0.0                  0.0   \n",
       "2             0.0            0.0               0.0                  0.0   \n",
       "3             0.0            0.0               0.0                  0.0   \n",
       "4             0.0            0.0               0.0                  0.0   \n",
       "5             0.0            0.0               0.0                  0.0   \n",
       "6             0.0            0.0               0.0                  0.0   \n",
       "7             0.0            0.0               0.0                  0.0   \n",
       "8             0.0            0.0               0.0                  0.0   \n",
       "9             0.0            0.0               0.0                  0.0   \n",
       "\n",
       "   meabp_min  ...  renal  respiration_24hours  coagulation_24hours  \\\n",
       "0   0.000000  ...    0.0                  0.5                  0.0   \n",
       "1   0.147887  ...    0.0                  1.0                  0.0   \n",
       "2   0.866197  ...    0.0                  1.0                  0.0   \n",
       "3   0.133803  ...    0.0                  1.0                  0.0   \n",
       "4   0.008621  ...    0.0                  0.0                  1.0   \n",
       "5   0.212644  ...    0.0                  0.0                  1.0   \n",
       "6   0.540230  ...    0.0                  1.0                  1.0   \n",
       "7   0.413793  ...    0.0                  1.0                  1.0   \n",
       "8   0.000000  ...    0.0                  0.0                  0.0   \n",
       "9   0.429630  ...    0.0                  0.0                  0.0   \n",
       "\n",
       "   liver_24hours  cns_24hours  renal_24hours  sofa_24hours  infection_time  \\\n",
       "0            0.0          0.0            0.0          0.50               0   \n",
       "1            0.0          0.0            0.0          1.00               0   \n",
       "2            0.0          0.0            0.0          1.00               0   \n",
       "3            0.0          0.0            0.0          1.00               0   \n",
       "4            0.0          1.0            0.0          0.50               0   \n",
       "5            0.0          1.0            0.0          0.50               0   \n",
       "6            0.0          1.0            0.0          1.00               0   \n",
       "7            0.0          0.0            0.0          0.75               0   \n",
       "8            0.0          0.0            0.0          1.00               0   \n",
       "9            0.0          0.0            0.0          1.00               0   \n",
       "\n",
       "   is_infection  infection_hour  \n",
       "0           0.0             0.0  \n",
       "1           0.0             0.0  \n",
       "2           0.0             0.0  \n",
       "3           0.0             0.0  \n",
       "4           0.0             0.0  \n",
       "5           0.0             0.0  \n",
       "6           0.0             0.0  \n",
       "7           0.0             0.0  \n",
       "8           0.0             0.0  \n",
       "9           0.0             0.0  \n",
       "\n",
       "[10 rows x 30 columns]"
      ]
     },
     "execution_count": 585,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 586,
   "metadata": {},
   "outputs": [],
   "source": [
    "total=total.set_index('Unnamed: 0')\n",
    "# total=total.sort_values(by=['Unnamed: 0'])\n",
    "total = total.drop(['subject_id', 'stay_id', 'endtime', 'infection_time', 'infection_hour', 'compiled_hr'], axis=1)\n",
    "#total = total.astype('float')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 587,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatetimeIndex(['2112-02-16 19:00:00', '2112-02-17 03:00:00',\n",
      "               '2112-02-17 11:00:00', '2112-02-17 19:00:00',\n",
      "               '2131-10-11 14:00:00', '2131-10-11 22:00:00',\n",
      "               '2131-10-12 06:00:00', '2131-10-12 14:00:00',\n",
      "               '2128-10-15 03:00:00', '2128-10-15 11:00:00',\n",
      "               ...\n",
      "               '2141-05-06 18:00:00', '2141-05-07 02:00:00',\n",
      "               '2194-09-13 02:00:00', '2194-09-13 10:00:00',\n",
      "               '2194-09-13 18:00:00', '2194-09-14 02:00:00',\n",
      "               '2114-10-09 07:00:00', '2114-10-09 15:00:00',\n",
      "               '2114-10-09 23:00:00', '2114-10-10 07:00:00'],\n",
      "              dtype='datetime64[ns]', name='Unnamed: 0', length=4804, freq=None)\n"
     ]
    }
   ],
   "source": [
    "total.columns\n",
    "print(total.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 588,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1.  0.  0.  ... 0.  0.5 0. ]\n",
      " [0.  0.  0.  ... 0.  1.  0. ]\n",
      " [0.  0.  0.  ... 0.  1.  0. ]\n",
      " ...\n",
      " [0.  0.  0.  ... 1.  1.  0. ]\n",
      " [0.  0.  0.  ... 1.  0.5 0. ]\n",
      " [0.  0.  0.  ... 0.  0.  1. ]]\n"
     ]
    }
   ],
   "source": [
    "print(total.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 589,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['pao2ratio_vent', 'rate_dopamine', 'rate_epinephrine',\n",
      "       'rate_nonepinephrine', 'meabp_min', 'heartrate_max', 'temperature_max',\n",
      "       'gcs_min', 'bilirubin_max', 'creatineine_max', 'paltelet_min',\n",
      "       'respiration', 'coagulation', 'liver', 'cns', 'renal',\n",
      "       'respiration_24hours', 'coagulation_24hours', 'liver_24hours',\n",
      "       'cns_24hours', 'renal_24hours', 'sofa_24hours', 'is_infection'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "print(total.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xTdQQjnK5ZzF"
   },
   "source": [
    "# Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 590,
   "metadata": {
    "id": "d_zlRdiSTAgn"
   },
   "outputs": [],
   "source": [
    "class SepsisDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, df_data, seq_len=4):\n",
    "        self.seq_len = seq_len\n",
    "        self.X = df_data.loc[:,df_data.columns!='is_infection'].values\n",
    "        self.y = df_data.loc[:, 'is_infection'].values\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.X)//self.seq_len\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        idx += self.seq_len # 몇 시간 단위로 인덱스 올리는 거\n",
    "        X = self.X[idx-self.seq_len:idx-2]\n",
    "        y = self.y[idx-1]\n",
    "        return X, y "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 591,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1000, 2, 22]) torch.Size([1000])\n"
     ]
    }
   ],
   "source": [
    "train_dataset = SepsisDataset(total, seq_len=4)\n",
    "trainloader = DataLoader(train_dataset,  # dataset을 input으로\n",
    "                             batch_size=1000,\n",
    "                             shuffle=True, drop_last=True)\n",
    "\n",
    "for i, (X, y) in enumerate(trainloader) :\n",
    "    print(X.size() , y.size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Al40scAi5ZzG"
   },
   "source": [
    "# Model Define"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 592,
   "metadata": {
    "id": "Rr7F5kpl5ZzH"
   },
   "outputs": [],
   "source": [
    "class LSTM(nn.Module):\n",
    "    \n",
    "    def __init__(self, input_dim, hidden_dim, num_layers, batch_size, dropout, use_bn):\n",
    "        super(LSTM, self).__init__()\n",
    "        self.input_dim = input_dim \n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.output_dim = 1\n",
    "        self.num_layers = num_layers\n",
    "\n",
    "        self.batch_size = batch_size\n",
    "        self.dropout = dropout\n",
    "        self.use_bn = use_bn \n",
    "        \n",
    "        self.lstm = nn.LSTM(self.input_dim, self.hidden_dim, self.num_layers)\n",
    "        self.hidden = self.init_hidden()\n",
    "        self.regressor = self.make_regressor()\n",
    "        \n",
    "    def init_hidden(self):\n",
    "        return (torch.zeros(self.num_layers, self.batch_size, self.hidden_dim),\n",
    "                torch.zeros(self.num_layers, self.batch_size, self.hidden_dim))\n",
    "    \n",
    "    def make_regressor(self):\n",
    "        layers = []\n",
    "        if self.use_bn:\n",
    "            layers.append(nn.BatchNorm1d(self.hidden_dim))\n",
    "        layers.append(nn.Dropout(self.dropout))\n",
    "        \n",
    "        # 나중에 network(node 개수 바꿔 주는 옵션도 생각하기)\n",
    "        layers.append(nn.Linear(self.hidden_dim, self.hidden_dim // 2))\n",
    "        layers.append(nn.ReLU())\n",
    "        layers.append(nn.Linear(self.hidden_dim // 2, self.output_dim))\n",
    "        layers.append(nn.Sigmoid())\n",
    "        regressor = nn.Sequential(*layers)\n",
    "        return regressor\n",
    "    \n",
    "    def forward(self, x):\n",
    "      # Model Capacity 따라 y.sahpe이 달라짐\n",
    "        lstm_out, self.hidden = self.lstm(x, self.hidden)\n",
    "        y_pred = self.regressor(lstm_out[-1].view(self.batch_size, -1)) # 맨 마지막 h_t 갖고 예측 할거니까 -1\n",
    "#         t = Variable(torch.Tensor([0.5]))  # threshold\n",
    "#         out = (y_pred > t).float() * 1\n",
    "        return y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 593,
   "metadata": {
    "id": "pqir1JP_5ZzH"
   },
   "outputs": [],
   "source": [
    "def metric(y_pred, y_true):  \n",
    "#     auc = roc_auc_score(y_true, y_pred) \n",
    "    \n",
    "    perc_y_pred = y_pred.cpu().detach().numpy()\n",
    "    perc_y_true = y_true.cpu().detach().numpy()\n",
    "\n",
    "#     try:\n",
    "#         auc = roc_auc_score(perc_y_true, perc_y_pred)\n",
    "#     except ValueError:\n",
    "#         print('Error')\n",
    "#         auc = 0.99\n",
    "    perc_y_pred = np.where(perc_y_pred > 0.5, 1, 0)\n",
    "#     print('@@@@@@@@@@@@@')\n",
    "#     print(perc_y_pred)\n",
    "#     print(perc_y_true)\n",
    "    auc = accuracy_score(perc_y_true, perc_y_pred, normalize=True)\n",
    "#     print(auc)\n",
    "    return auc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0j20Sve95ZzH"
   },
   "source": [
    "# Train, Validate, Test "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 594,
   "metadata": {
    "id": "I7FqJ9EX5ZzI"
   },
   "outputs": [],
   "source": [
    "def train(model, partition, optimizer, loss_fn, args):\n",
    "  # batch 단위로 갖고 오는 것\n",
    "  # 무작위로 시작 record 갖고 옴\n",
    "  # 갖고 올때 batch size만큼 갖고 옴\n",
    "  # permutation, not random\n",
    "    trainloader = DataLoader(partition['train'],  # dataset을 input으로\n",
    "                             batch_size=args.batch_size,\n",
    "                             shuffle=True, drop_last=True)\n",
    "    model.train()\n",
    "    model.zero_grad()\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    train_acc = 0.0\n",
    "    train_loss = 0.0\n",
    "    for i, (X, y) in enumerate(trainloader):\n",
    "\n",
    "        # X : [10, n, 6] [n, 10, 6] 현재 타임스템프 안에\n",
    "        # Y : [10, m, 1] \n",
    "        X = X.transpose(0, 1).float().to(args.device)\n",
    "        y_true = y.float().to(args.device) \n",
    "\n",
    "        model.zero_grad()\n",
    "        optimizer.zero_grad()\n",
    "        model.hidden = [hidden.to(args.device) for hidden in model.init_hidden()]\n",
    "\n",
    "        y_pred = model(X)\n",
    "        loss = loss_fn(y_pred.view(-1), y_true.view(-1))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        train_loss += loss.item()\n",
    "        train_acc += metric(y_pred, y_true)\n",
    "\n",
    "    train_loss = train_loss / len(trainloader)\n",
    "    train_acc = train_acc / len(trainloader)\n",
    "    \n",
    "    return model, train_loss, train_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 595,
   "metadata": {
    "id": "JTX3hfCI5ZzI"
   },
   "outputs": [],
   "source": [
    "def validate(model, partition, loss_fn, args):\n",
    "    valloader = DataLoader(partition['val'], \n",
    "                           batch_size=args.batch_size, \n",
    "                           shuffle=False, drop_last=True)\n",
    "    model.eval()\n",
    "\n",
    "    val_acc = 0.0\n",
    "    val_loss = 0.0\n",
    "    with torch.no_grad():\n",
    "        for i, (X, y) in enumerate(valloader):\n",
    "\n",
    "            X = X.transpose(0, 1).float().to(args.device)\n",
    "            y_true = y.float().to(args.device)\n",
    "            model.hidden = [hidden.to(args.device) for hidden in model.init_hidden()]\n",
    "\n",
    "            y_pred = model(X)\n",
    "            loss = loss_fn(y_pred.view(-1), y_true.view(-1))\n",
    "\n",
    "            val_loss += loss.item()\n",
    "            val_acc += metric(y_pred, y_true)\n",
    "\n",
    "    val_loss = val_loss / len(valloader)\n",
    "    val_acc = val_acc / len(valloader)\n",
    "    return val_loss, val_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 596,
   "metadata": {
    "id": "kAmbRVca5ZzI"
   },
   "outputs": [],
   "source": [
    "def test(model, partition, args):\n",
    "    testloader = DataLoader(partition['test'], \n",
    "                           batch_size=args.batch_size, \n",
    "                           shuffle=False, drop_last=True)\n",
    "    model.eval()\n",
    "\n",
    "    test_acc = 0.0\n",
    "    with torch.no_grad():\n",
    "        for i, (X, y) in enumerate(testloader):\n",
    "\n",
    "            X = X.transpose(0, 1).float().to(args.device)\n",
    "            y_true = y.float().to(args.device)\n",
    "            model.hidden = [hidden.to(args.device) for hidden in model.init_hidden()]\n",
    "\n",
    "            y_pred = model(X)\n",
    "            test_acc += metric(y_pred, y_true)\n",
    "\n",
    "    test_acc = test_acc / len(testloader)\n",
    "    return y_pred, test_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 597,
   "metadata": {
    "id": "RgQSze2O5ZzJ"
   },
   "outputs": [],
   "source": [
    "def experiment(partition, args):\n",
    "\n",
    "    model = LSTM(args.input_dim, args.hid_dim, args.n_layers, args.batch_size, args.dropout, args.use_bn)\n",
    "    model.to(args.device)\n",
    "\n",
    "    # loss_fn = torch.nn.BCELoss(weight=None, size_average=None, reduce=None, reduction='mean')\n",
    "    loss_fn = nn.BCELoss()\n",
    "    if args.optim == 'SGD':\n",
    "        optimizer = optim.RMSprop(model.parameters(), lr=args.lr, weight_decay=args.l2)\n",
    "    elif args.optim == 'RMSprop':\n",
    "        optimizer = optim.RMSprop(model.parameters(), lr=args.lr, weight_decay=args.l2)\n",
    "    elif args.optim == 'Adam':\n",
    "        optimizer = optim.Adam(model.parameters(), lr=args.lr, weight_decay=args.l2)\n",
    "    else:\n",
    "        raise ValueError('In-valid optimizer choice')\n",
    "    \n",
    "    # ===== List for epoch-wise data ====== #\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "    train_accs = []\n",
    "    val_accs = []\n",
    "    # ===================================== #\n",
    "        \n",
    "    for epoch in range(args.epoch):  # loop over the dataset multiple times\n",
    "        ts = time.time()\n",
    "        model, train_loss, train_acc = train(model, partition, optimizer, loss_fn, args)\n",
    "        val_loss, val_acc = validate(model, partition, loss_fn, args)\n",
    "        te = time.time()\n",
    "        \n",
    "        # ====== Add Epoch Data ====== #\n",
    "        train_losses.append(train_loss)\n",
    "        val_losses.append(val_loss)\n",
    "        train_accs.append(train_acc)\n",
    "        val_accs.append(val_acc)\n",
    "        # ============================ #\n",
    "        \n",
    "        print('Epoch {}, Acc(train/val): {:2.2f}/{:2.2f}, Loss(train/val) {:2.5f}/{:2.5f}. Took {:2.2f} sec'.format(epoch, train_acc, val_acc, train_loss, val_loss, te-ts))\n",
    "        \n",
    "    test_acc = test(model, partition, args)    \n",
    "    \n",
    "    # ======= Add Result to Dictionary ======= #\n",
    "    result = {}\n",
    "    result['train_losses'] = train_losses\n",
    "    result['val_losses'] = val_losses\n",
    "    result['train_accs'] = train_accs\n",
    "    result['val_accs'] = val_accs\n",
    "    result['train_acc'] = train_acc\n",
    "    result['val_acc'] = val_acc\n",
    "    result['test_acc'] = test_acc\n",
    "    return vars(args), result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7OUVge945ZzJ"
   },
   "source": [
    "# Manage Experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 598,
   "metadata": {
    "id": "6Ved9MOK5ZzJ"
   },
   "outputs": [],
   "source": [
    "import hashlib\n",
    "import json\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "import pandas as pd\n",
    "\n",
    "def save_exp_result(setting, result):\n",
    "    exp_name = setting['exp_name']\n",
    "    del setting['epoch']\n",
    "\n",
    "    hash_key = hashlib.sha1(str(setting).encode()).hexdigest()[:6]\n",
    "    filename = './results/{}-{}.json'.format(exp_name, hash_key)\n",
    "    result.update(setting)\n",
    "    with open(filename, 'w') as f:\n",
    "        json.dump(result, f)\n",
    "\n",
    "    \n",
    "def load_exp_result(exp_name):\n",
    "    dir_path = './results'\n",
    "    filenames = [f for f in listdir(dir_path) if isfile(join(dir_path, f)) if '.json' in f]\n",
    "    list_result = []\n",
    "    for filename in filenames:\n",
    "        if exp_name in filename:\n",
    "            with open(join(dir_path, filename), 'r') as infile:\n",
    "                results = json.load(infile)\n",
    "                list_result.append(results)\n",
    "    df = pd.DataFrame(list_result) # .drop(columns=[])\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 599,
   "metadata": {
    "id": "kfLW-ZBo5ZzJ"
   },
   "outputs": [],
   "source": [
    "\n",
    "def plot_acc(var1, var2, df):\n",
    "\n",
    "    fig, ax = plt.subplots(1, 3)\n",
    "    fig.set_size_inches(15, 6)\n",
    "    sns.set_style(\"darkgrid\", {\"axes.facecolor\": \".9\"})\n",
    "\n",
    "    sns.barplot(x=var1, y='train_acc', hue=var2, data=df, ax=ax[0])\n",
    "    sns.barplot(x=var1, y='val_acc', hue=var2, data=df, ax=ax[1])\n",
    "    sns.barplot(x=var1, y='test_acc', hue=var2, data=df, ax=ax[2])\n",
    "    \n",
    "    ax[0].set_title('Train Accuracy')\n",
    "    ax[1].set_title('Validation Accuracy')\n",
    "    ax[2].set_title('Test Accuracy')\n",
    "\n",
    "    \n",
    "def plot_loss_variation(var1, var2, df, **kwargs):\n",
    "\n",
    "    list_v1 = df[var1].unique()\n",
    "    list_v2 = df[var2].unique()\n",
    "    list_data = []\n",
    "\n",
    "    for value1 in list_v1:\n",
    "        for value2 in list_v2:\n",
    "            row = df.loc[df[var1]==value1]\n",
    "            row = row.loc[df[var2]==value2]\n",
    "\n",
    "            train_losses = list(row.train_losses)[0]\n",
    "            val_losses = list(row.val_losses)[0]\n",
    "\n",
    "            for epoch, train_loss in enumerate(train_losses):\n",
    "                list_data.append({'type':'train', 'loss':train_loss, 'epoch':epoch, var1:value1, var2:value2})\n",
    "            for epoch, val_loss in enumerate(val_losses):\n",
    "                list_data.append({'type':'val', 'loss':val_loss, 'epoch':epoch, var1:value1, var2:value2})\n",
    "\n",
    "    df = pd.DataFrame(list_data)\n",
    "    g = sns.FacetGrid(df, row=var2, col=var1, hue='type', **kwargs)\n",
    "    g = g.map(plt.plot, 'epoch', 'loss', marker='.')\n",
    "    g.add_legend()\n",
    "    g.fig.suptitle('Train loss vs Val loss')\n",
    "    plt.subplots_adjust(top=0.89) # 만약 Title이 그래프랑 겹친다면 top 값을 조정해주면 됩니다! 함수 인자로 받으면 그래프마다 조절할 수 있겠죠?\n",
    "\n",
    "\n",
    "def plot_acc_variation(var1, var2, df, **kwargs):\n",
    "    list_v1 = df[var1].unique()\n",
    "    list_v2 = df[var2].unique()\n",
    "    list_data = []\n",
    "\n",
    "    for value1 in list_v1:\n",
    "        for value2 in list_v2:\n",
    "            row = df.loc[df[var1]==value1]\n",
    "            row = row.loc[df[var2]==value2]\n",
    "\n",
    "            train_accs = list(row.train_accs)[0]\n",
    "            val_accs = list(row.val_accs)[0]\n",
    "            test_acc = list(row.test_acc)[0]\n",
    "\n",
    "            for epoch, train_acc in enumerate(train_accs):\n",
    "                list_data.append({'type':'train', 'Acc':train_acc, 'test_acc':test_acc, 'epoch':epoch, var1:value1, var2:value2})\n",
    "            for epoch, val_acc in enumerate(val_accs):\n",
    "                list_data.append({'type':'val', 'Acc':val_acc, 'test_acc':test_acc, 'epoch':epoch, var1:value1, var2:value2})\n",
    "\n",
    "    df = pd.DataFrame(list_data)\n",
    "    g = sns.FacetGrid(df, row=var2, col=var1, hue='type', **kwargs)\n",
    "    g = g.map(plt.plot, 'epoch', 'Acc', marker='.')\n",
    "\n",
    "    def show_acc(x, y, metric, **kwargs):\n",
    "        plt.scatter(x, y, alpha=0.3, s=1)\n",
    "        metric = \"Test Acc: {:1.3f}\".format(list(metric.values)[0])\n",
    "        plt.text(0.05, 0.95, metric,  horizontalalignment='left', verticalalignment='center', transform=plt.gca().transAxes, bbox=dict(facecolor='yellow', alpha=0.5, boxstyle=\"round,pad=0.1\"))\n",
    "    g = g.map(show_acc, 'epoch', 'Acc', 'test_acc')\n",
    "\n",
    "    g.add_legend()\n",
    "    g.fig.suptitle('Train Accuracy vs Val Accuracy')\n",
    "    plt.subplots_adjust(top=0.89)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 600,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_val_dataset = total[:int((len(total) / 4)*.8)*4]\n",
    "test_dataset = total[int((len(total) / 4)*.8)*4:]\n",
    "train_dataset = train_val_dataset[:int((len(train_val_dataset) / 4)*.8)*4]\n",
    "valid_dataset = train_val_dataset[int((len(train_val_dataset) / 4)*.8)*4:]\n",
    "\n",
    "trainset = SepsisDataset(train_dataset, seq_len=4)\n",
    "valset = SepsisDataset(valid_dataset, seq_len=4)\n",
    "testset = SepsisDataset(test_dataset, seq_len=4)\n",
    "\n",
    "partition = {'train': trainset, 'val':valset, 'test':testset}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 603,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 375
    },
    "id": "D2umJYQT5ZzK",
    "outputId": "96e326e4-74a7-4428-b5f5-213e83ae931c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Namespace(batch_size=126, device='cuda', dropout=0.0, epoch=50, exp_name='exp1_lr', hid_dim=50, input_dim=22, l2=1e-05, lr=0.001, n_layers=1, optim='Adam', use_bn=True)\n",
      "Epoch 0, Acc(train/val): 0.68/0.95, Loss(train/val) 0.65822/0.63701. Took 0.03 sec\n",
      "Epoch 1, Acc(train/val): 0.90/0.95, Loss(train/val) 0.58648/0.62428. Took 0.03 sec\n",
      "Epoch 2, Acc(train/val): 0.96/0.95, Loss(train/val) 0.52861/0.60956. Took 0.03 sec\n",
      "Epoch 3, Acc(train/val): 0.96/0.95, Loss(train/val) 0.47919/0.59075. Took 0.03 sec\n",
      "Epoch 4, Acc(train/val): 0.96/0.95, Loss(train/val) 0.43488/0.56587. Took 0.03 sec\n",
      "Epoch 5, Acc(train/val): 0.96/0.95, Loss(train/val) 0.39427/0.53163. Took 0.03 sec\n",
      "Epoch 6, Acc(train/val): 0.96/0.95, Loss(train/val) 0.35364/0.48731. Took 0.03 sec\n",
      "Epoch 7, Acc(train/val): 0.96/0.95, Loss(train/val) 0.32075/0.43593. Took 0.03 sec\n",
      "Epoch 8, Acc(train/val): 0.96/0.95, Loss(train/val) 0.29091/0.38343. Took 0.03 sec\n",
      "Epoch 9, Acc(train/val): 0.96/0.95, Loss(train/val) 0.26547/0.33915. Took 0.03 sec\n",
      "Epoch 10, Acc(train/val): 0.96/0.95, Loss(train/val) 0.24019/0.30654. Took 0.03 sec\n",
      "Epoch 11, Acc(train/val): 0.96/0.95, Loss(train/val) 0.22346/0.28403. Took 0.03 sec\n",
      "Epoch 12, Acc(train/val): 0.96/0.95, Loss(train/val) 0.21025/0.26910. Took 0.03 sec\n",
      "Epoch 13, Acc(train/val): 0.96/0.95, Loss(train/val) 0.19529/0.25841. Took 0.03 sec\n",
      "Epoch 14, Acc(train/val): 0.96/0.95, Loss(train/val) 0.18542/0.25189. Took 0.03 sec\n",
      "Epoch 15, Acc(train/val): 0.96/0.95, Loss(train/val) 0.17783/0.24586. Took 0.03 sec\n",
      "Epoch 16, Acc(train/val): 0.96/0.95, Loss(train/val) 0.16797/0.23955. Took 0.03 sec\n",
      "Epoch 17, Acc(train/val): 0.96/0.95, Loss(train/val) 0.15969/0.23495. Took 0.03 sec\n",
      "Epoch 18, Acc(train/val): 0.96/0.95, Loss(train/val) 0.15856/0.22949. Took 0.03 sec\n",
      "Epoch 19, Acc(train/val): 0.96/0.95, Loss(train/val) 0.14948/0.22700. Took 0.03 sec\n",
      "Epoch 20, Acc(train/val): 0.96/0.95, Loss(train/val) 0.14612/0.22473. Took 0.03 sec\n",
      "Epoch 21, Acc(train/val): 0.96/0.95, Loss(train/val) 0.14561/0.22157. Took 0.03 sec\n",
      "Epoch 22, Acc(train/val): 0.96/0.95, Loss(train/val) 0.14111/0.21767. Took 0.03 sec\n",
      "Epoch 23, Acc(train/val): 0.96/0.95, Loss(train/val) 0.13555/0.21486. Took 0.03 sec\n",
      "Epoch 24, Acc(train/val): 0.96/0.95, Loss(train/val) 0.13284/0.21222. Took 0.03 sec\n",
      "Epoch 25, Acc(train/val): 0.96/0.95, Loss(train/val) 0.12805/0.21075. Took 0.03 sec\n",
      "Epoch 26, Acc(train/val): 0.96/0.95, Loss(train/val) 0.13024/0.20991. Took 0.03 sec\n",
      "Epoch 27, Acc(train/val): 0.96/0.95, Loss(train/val) 0.13050/0.21012. Took 0.03 sec\n",
      "Epoch 28, Acc(train/val): 0.96/0.95, Loss(train/val) 0.12752/0.20499. Took 0.03 sec\n",
      "Epoch 29, Acc(train/val): 0.96/0.95, Loss(train/val) 0.12316/0.20246. Took 0.03 sec\n",
      "Epoch 30, Acc(train/val): 0.96/0.95, Loss(train/val) 0.12165/0.20281. Took 0.03 sec\n",
      "Epoch 31, Acc(train/val): 0.96/0.95, Loss(train/val) 0.12060/0.20099. Took 0.03 sec\n",
      "Epoch 32, Acc(train/val): 0.96/0.95, Loss(train/val) 0.11421/0.20064. Took 0.03 sec\n",
      "Epoch 33, Acc(train/val): 0.96/0.95, Loss(train/val) 0.11392/0.19882. Took 0.04 sec\n",
      "Epoch 34, Acc(train/val): 0.96/0.95, Loss(train/val) 0.10693/0.19957. Took 0.03 sec\n",
      "Epoch 35, Acc(train/val): 0.96/0.95, Loss(train/val) 0.10977/0.20034. Took 0.03 sec\n",
      "Epoch 36, Acc(train/val): 0.96/0.95, Loss(train/val) 0.11056/0.19906. Took 0.04 sec\n",
      "Epoch 37, Acc(train/val): 0.96/0.95, Loss(train/val) 0.10865/0.19840. Took 0.03 sec\n",
      "Epoch 38, Acc(train/val): 0.96/0.95, Loss(train/val) 0.10734/0.19983. Took 0.04 sec\n",
      "Epoch 39, Acc(train/val): 0.96/0.95, Loss(train/val) 0.10508/0.19741. Took 0.04 sec\n",
      "Epoch 40, Acc(train/val): 0.96/0.95, Loss(train/val) 0.10371/0.19285. Took 0.05 sec\n",
      "Epoch 41, Acc(train/val): 0.96/0.95, Loss(train/val) 0.10182/0.19157. Took 0.05 sec\n",
      "Epoch 42, Acc(train/val): 0.96/0.95, Loss(train/val) 0.10140/0.19392. Took 0.05 sec\n",
      "Epoch 43, Acc(train/val): 0.96/0.95, Loss(train/val) 0.09419/0.19479. Took 0.05 sec\n",
      "Epoch 44, Acc(train/val): 0.96/0.95, Loss(train/val) 0.09555/0.19017. Took 0.05 sec\n",
      "Epoch 45, Acc(train/val): 0.96/0.96, Loss(train/val) 0.09648/0.19030. Took 0.05 sec\n",
      "Epoch 46, Acc(train/val): 0.96/0.95, Loss(train/val) 0.09568/0.19149. Took 0.05 sec\n",
      "Epoch 47, Acc(train/val): 0.96/0.94, Loss(train/val) 0.09244/0.19298. Took 0.04 sec\n",
      "Epoch 48, Acc(train/val): 0.97/0.94, Loss(train/val) 0.09132/0.19234. Took 0.04 sec\n",
      "Epoch 49, Acc(train/val): 0.97/0.94, Loss(train/val) 0.09225/0.18955. Took 0.04 sec\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "Object of type 'Tensor' is not JSON serializable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-603-9ed17eae4cf7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m         \u001b[0msetting\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mexperiment\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpartition\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdeepcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 41\u001b[0;31m         \u001b[0msave_exp_result\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msetting\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-598-261485856116>\u001b[0m in \u001b[0;36msave_exp_result\u001b[0;34m(setting, result)\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0mresult\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msetting\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'w'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m         \u001b[0mjson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdump\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.6/json/__init__.py\u001b[0m in \u001b[0;36mdump\u001b[0;34m(obj, fp, skipkeys, ensure_ascii, check_circular, allow_nan, cls, indent, separators, default, sort_keys, **kw)\u001b[0m\n\u001b[1;32m    177\u001b[0m     \u001b[0;31m# could accelerate with writelines in some versions of Python, at\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    178\u001b[0m     \u001b[0;31m# a debuggability cost\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 179\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mchunk\u001b[0m \u001b[0;32min\u001b[0m \u001b[0miterable\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    180\u001b[0m         \u001b[0mfp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchunk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    181\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.6/json/encoder.py\u001b[0m in \u001b[0;36m_iterencode\u001b[0;34m(o, _current_indent_level)\u001b[0m\n\u001b[1;32m    428\u001b[0m             \u001b[0;32myield\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0m_iterencode_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mo\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_current_indent_level\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    429\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mo\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 430\u001b[0;31m             \u001b[0;32myield\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0m_iterencode_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mo\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_current_indent_level\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    431\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    432\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mmarkers\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.6/json/encoder.py\u001b[0m in \u001b[0;36m_iterencode_dict\u001b[0;34m(dct, _current_indent_level)\u001b[0m\n\u001b[1;32m    402\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    403\u001b[0m                     \u001b[0mchunks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_iterencode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_current_indent_level\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 404\u001b[0;31m                 \u001b[0;32myield\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mchunks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    405\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mnewline_indent\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    406\u001b[0m             \u001b[0m_current_indent_level\u001b[0m \u001b[0;34m-=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.6/json/encoder.py\u001b[0m in \u001b[0;36m_iterencode_list\u001b[0;34m(lst, _current_indent_level)\u001b[0m\n\u001b[1;32m    323\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    324\u001b[0m                     \u001b[0mchunks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_iterencode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_current_indent_level\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 325\u001b[0;31m                 \u001b[0;32myield\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mchunks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    326\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mnewline_indent\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    327\u001b[0m             \u001b[0m_current_indent_level\u001b[0m \u001b[0;34m-=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.6/json/encoder.py\u001b[0m in \u001b[0;36m_iterencode\u001b[0;34m(o, _current_indent_level)\u001b[0m\n\u001b[1;32m    435\u001b[0m                     \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Circular reference detected\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    436\u001b[0m                 \u001b[0mmarkers\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmarkerid\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mo\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 437\u001b[0;31m             \u001b[0mo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_default\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mo\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    438\u001b[0m             \u001b[0;32myield\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0m_iterencode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mo\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_current_indent_level\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    439\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mmarkers\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.6/json/encoder.py\u001b[0m in \u001b[0;36mdefault\u001b[0;34m(self, o)\u001b[0m\n\u001b[1;32m    178\u001b[0m         \"\"\"\n\u001b[1;32m    179\u001b[0m         raise TypeError(\"Object of type '%s' is not JSON serializable\" %\n\u001b[0;32m--> 180\u001b[0;31m                         o.__class__.__name__)\n\u001b[0m\u001b[1;32m    181\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    182\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mencode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mo\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: Object of type 'Tensor' is not JSON serializable"
     ]
    }
   ],
   "source": [
    "# ====== Random Seed Initialization ====== #\n",
    "seed = 666\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "\n",
    "parser = argparse.ArgumentParser()\n",
    "args = parser.parse_args(\"\")\n",
    "args.exp_name = \"exp1_lr\"\n",
    "args.device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "args.batch_size = 126\n",
    "\n",
    "# ====== Model Capacity ===== #\n",
    "args.input_dim = 22\n",
    "args.hid_dim = 50\n",
    "args.n_layers = 2\n",
    "\n",
    "# ====== Regularization ======= #\n",
    "args.l2 = 0.00001\n",
    "args.dropout = 0.0\n",
    "args.use_bn = True\n",
    "\n",
    "# ====== Optimizer & Training ====== #\n",
    "args.optim = 'Adam' #'RMSprop' #SGD, RMSprop, ADAM...\n",
    "args.lr = 0.0001\n",
    "args.epoch = 50\n",
    "\n",
    "\n",
    "# ====== Experiment Variable ====== #\n",
    "name_var1 = 'lr'\n",
    "name_var2 = 'n_layers'\n",
    "list_var1 = [0.001, 0.0001, 0.00001]\n",
    "list_var2 = [1,2,3]\n",
    "\n",
    "for var1 in list_var1:\n",
    "    for var2 in list_var2:\n",
    "        setattr(args, name_var1, var1)\n",
    "        setattr(args, name_var2, var2)\n",
    "        print(args)\n",
    "                \n",
    "        setting, result = experiment(partition, deepcopy(args))\n",
    "        save_exp_result(setting, result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "N-lGOJcT5ZzK"
   },
   "outputs": [],
   "source": [
    "var1 = 'lr'\n",
    "var2 = 'n_layers'\n",
    "df = load_exp_result('exp1')\n",
    "\n",
    "plot_acc(var1, var2, df)\n",
    "plot_loss_variation(var1, var2, df, sharey=False) #sharey를 True로 하면 모둔 subplot의 y축의 스케일이 같아집니다.\n",
    "plot_acc_variation(var1, var2, df, margin_titles=True, sharey=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pKevCSOB5ZzK"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Sepsis_with_LSTM.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
