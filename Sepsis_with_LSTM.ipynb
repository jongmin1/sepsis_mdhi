{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Gqlpt6bO5ZzD",
    "outputId": "04886b08-66a6-4076-c8a9-7d78da19622b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.9.0+cu111\n",
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import datetime\n",
    "import torch\n",
    "from torch.autograd import Variable\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import argparse\n",
    "from copy import deepcopy # Add Deepcopy for args\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.metrics import roc_auc_score\n",
    "import seaborn as sns \n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.preprocessing import binarize\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, precision_score, recall_score,f1_score ,roc_auc_score, explained_variance_score, mean_squared_error, r2_score, plot_roc_curve\n",
    "\n",
    "print(torch.__version__)\n",
    "%matplotlib inline\n",
    "%pylab inline\n",
    "pylab.rcParams['figure.figsize'] = (15, 9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "__DRQb_p5ZzE"
   },
   "source": [
    "# Pandas Datareader Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4804\n"
     ]
    }
   ],
   "source": [
    "total = pd.read_csv('./result/total.csv')\n",
    "print(len(total))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4804\n",
      "0.46331156358048403\n"
     ]
    }
   ],
   "source": [
    "print(len(total))\n",
    "temp = total[total['is_infection']==1]\n",
    "total = pd.concat([total, temp])\n",
    "total = pd.concat([total, temp])\n",
    "e = len(total[total['is_infection']==1])\n",
    "print(e*4/len(total))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.series.Series'>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Index(['Unnamed: 0', 'subject_id', 'stay_id', 'compiled_hr', 'endtime',\n",
       "       'pao2ratio_vent', 'rate_dopamine', 'rate_epinephrine',\n",
       "       'rate_nonepinephrine', 'meabp_min', 'heartrate_max', 'temperature_max',\n",
       "       'gcs_min', 'bilirubin_max', 'creatineine_max', 'paltelet_min',\n",
       "       'respiration', 'coagulation', 'liver', 'cns', 'renal',\n",
       "       'respiration_24hours', 'coagulation_24hours', 'liver_24hours',\n",
       "       'cns_24hours', 'renal_24hours', 'sofa_24hours', 'infection_time',\n",
       "       'is_infection', 'infection_hour'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total.columns\n",
    "# print(type(total))\n",
    "print(type(total['Unnamed: 0']))\n",
    "total['Unnamed: 0'] = pd.to_datetime(total['Unnamed: 0'])\n",
    "total.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>subject_id</th>\n",
       "      <th>stay_id</th>\n",
       "      <th>compiled_hr</th>\n",
       "      <th>endtime</th>\n",
       "      <th>pao2ratio_vent</th>\n",
       "      <th>rate_dopamine</th>\n",
       "      <th>rate_epinephrine</th>\n",
       "      <th>rate_nonepinephrine</th>\n",
       "      <th>meabp_min</th>\n",
       "      <th>...</th>\n",
       "      <th>renal</th>\n",
       "      <th>respiration_24hours</th>\n",
       "      <th>coagulation_24hours</th>\n",
       "      <th>liver_24hours</th>\n",
       "      <th>cns_24hours</th>\n",
       "      <th>renal_24hours</th>\n",
       "      <th>sofa_24hours</th>\n",
       "      <th>infection_time</th>\n",
       "      <th>is_infection</th>\n",
       "      <th>infection_hour</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2112-02-16 19:00:00</td>\n",
       "      <td>14034916</td>\n",
       "      <td>39521325</td>\n",
       "      <td>0</td>\n",
       "      <td>2112-02-17 03:00:00</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2112-02-17 03:00:00</td>\n",
       "      <td>14034916</td>\n",
       "      <td>39521325</td>\n",
       "      <td>1</td>\n",
       "      <td>2112-02-17 11:00:00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.147887</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2112-02-17 11:00:00</td>\n",
       "      <td>14034916</td>\n",
       "      <td>39521325</td>\n",
       "      <td>2</td>\n",
       "      <td>2112-02-17 19:00:00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.866197</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2112-02-17 19:00:00</td>\n",
       "      <td>14034916</td>\n",
       "      <td>39521325</td>\n",
       "      <td>3</td>\n",
       "      <td>2112-02-18 03:00:00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.133803</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2131-10-11 14:00:00</td>\n",
       "      <td>12588462</td>\n",
       "      <td>32189053</td>\n",
       "      <td>0</td>\n",
       "      <td>2131-10-11 22:00:00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.008621</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2131-10-11 22:00:00</td>\n",
       "      <td>12588462</td>\n",
       "      <td>32189053</td>\n",
       "      <td>1</td>\n",
       "      <td>2131-10-12 06:00:00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.212644</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>2131-10-12 06:00:00</td>\n",
       "      <td>12588462</td>\n",
       "      <td>32189053</td>\n",
       "      <td>2</td>\n",
       "      <td>2131-10-12 14:00:00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.540230</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>2131-10-12 14:00:00</td>\n",
       "      <td>12588462</td>\n",
       "      <td>32189053</td>\n",
       "      <td>3</td>\n",
       "      <td>2131-10-12 22:00:00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.413793</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.75</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>2128-10-15 03:00:00</td>\n",
       "      <td>17188632</td>\n",
       "      <td>31948656</td>\n",
       "      <td>0</td>\n",
       "      <td>2128-10-15 11:00:00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>2128-10-15 11:00:00</td>\n",
       "      <td>17188632</td>\n",
       "      <td>31948656</td>\n",
       "      <td>1</td>\n",
       "      <td>2128-10-15 19:00:00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.429630</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10 rows × 30 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           Unnamed: 0  subject_id   stay_id  compiled_hr              endtime  \\\n",
       "0 2112-02-16 19:00:00    14034916  39521325            0  2112-02-17 03:00:00   \n",
       "1 2112-02-17 03:00:00    14034916  39521325            1  2112-02-17 11:00:00   \n",
       "2 2112-02-17 11:00:00    14034916  39521325            2  2112-02-17 19:00:00   \n",
       "3 2112-02-17 19:00:00    14034916  39521325            3  2112-02-18 03:00:00   \n",
       "4 2131-10-11 14:00:00    12588462  32189053            0  2131-10-11 22:00:00   \n",
       "5 2131-10-11 22:00:00    12588462  32189053            1  2131-10-12 06:00:00   \n",
       "6 2131-10-12 06:00:00    12588462  32189053            2  2131-10-12 14:00:00   \n",
       "7 2131-10-12 14:00:00    12588462  32189053            3  2131-10-12 22:00:00   \n",
       "8 2128-10-15 03:00:00    17188632  31948656            0  2128-10-15 11:00:00   \n",
       "9 2128-10-15 11:00:00    17188632  31948656            1  2128-10-15 19:00:00   \n",
       "\n",
       "   pao2ratio_vent  rate_dopamine  rate_epinephrine  rate_nonepinephrine  \\\n",
       "0             1.0            0.0               0.0                  0.0   \n",
       "1             0.0            0.0               0.0                  0.0   \n",
       "2             0.0            0.0               0.0                  0.0   \n",
       "3             0.0            0.0               0.0                  0.0   \n",
       "4             0.0            0.0               0.0                  0.0   \n",
       "5             0.0            0.0               0.0                  0.0   \n",
       "6             0.0            0.0               0.0                  0.0   \n",
       "7             0.0            0.0               0.0                  0.0   \n",
       "8             0.0            0.0               0.0                  0.0   \n",
       "9             0.0            0.0               0.0                  0.0   \n",
       "\n",
       "   meabp_min  ...  renal  respiration_24hours  coagulation_24hours  \\\n",
       "0   0.000000  ...    0.0                  0.5                  0.0   \n",
       "1   0.147887  ...    0.0                  1.0                  0.0   \n",
       "2   0.866197  ...    0.0                  1.0                  0.0   \n",
       "3   0.133803  ...    0.0                  1.0                  0.0   \n",
       "4   0.008621  ...    0.0                  0.0                  1.0   \n",
       "5   0.212644  ...    0.0                  0.0                  1.0   \n",
       "6   0.540230  ...    0.0                  1.0                  1.0   \n",
       "7   0.413793  ...    0.0                  1.0                  1.0   \n",
       "8   0.000000  ...    0.0                  0.0                  0.0   \n",
       "9   0.429630  ...    0.0                  0.0                  0.0   \n",
       "\n",
       "   liver_24hours  cns_24hours  renal_24hours  sofa_24hours  infection_time  \\\n",
       "0            0.0          0.0            0.0          0.50               0   \n",
       "1            0.0          0.0            0.0          1.00               0   \n",
       "2            0.0          0.0            0.0          1.00               0   \n",
       "3            0.0          0.0            0.0          1.00               0   \n",
       "4            0.0          1.0            0.0          0.50               0   \n",
       "5            0.0          1.0            0.0          0.50               0   \n",
       "6            0.0          1.0            0.0          1.00               0   \n",
       "7            0.0          0.0            0.0          0.75               0   \n",
       "8            0.0          0.0            0.0          1.00               0   \n",
       "9            0.0          0.0            0.0          1.00               0   \n",
       "\n",
       "   is_infection  infection_hour  \n",
       "0           0.0             0.0  \n",
       "1           0.0             0.0  \n",
       "2           0.0             0.0  \n",
       "3           0.0             0.0  \n",
       "4           0.0             0.0  \n",
       "5           0.0             0.0  \n",
       "6           0.0             0.0  \n",
       "7           0.0             0.0  \n",
       "8           0.0             0.0  \n",
       "9           0.0             0.0  \n",
       "\n",
       "[10 rows x 30 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "total=total.set_index('Unnamed: 0')\n",
    "# total=total.sort_values(by=['Unnamed: 0'])\n",
    "total = total.drop(['subject_id', 'stay_id', 'endtime', 'infection_time', 'infection_hour', 'compiled_hr'], axis=1)\n",
    "#total = total.astype('float')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatetimeIndex(['2112-02-16 19:00:00', '2112-02-17 03:00:00',\n",
      "               '2112-02-17 11:00:00', '2112-02-17 19:00:00',\n",
      "               '2131-10-11 14:00:00', '2131-10-11 22:00:00',\n",
      "               '2131-10-12 06:00:00', '2131-10-12 14:00:00',\n",
      "               '2128-10-15 03:00:00', '2128-10-15 11:00:00',\n",
      "               ...\n",
      "               '2143-11-30 13:00:00', '2184-06-17 05:00:00',\n",
      "               '2156-12-12 08:00:00', '2129-07-11 19:00:00',\n",
      "               '2118-01-14 05:00:00', '2180-02-21 07:00:00',\n",
      "               '2186-06-09 17:00:00', '2165-09-25 08:00:00',\n",
      "               '2181-01-10 05:00:00', '2114-10-10 07:00:00'],\n",
      "              dtype='datetime64[ns]', name='Unnamed: 0', length=5206, freq=None)\n"
     ]
    }
   ],
   "source": [
    "total.columns\n",
    "print(total.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1.  0.  0.  ... 0.  0.5 0. ]\n",
      " [0.  0.  0.  ... 0.  1.  0. ]\n",
      " [0.  0.  0.  ... 0.  1.  0. ]\n",
      " ...\n",
      " [0.  0.  0.  ... 0.  1.  1. ]\n",
      " [1.  0.  0.  ... 0.  0.1 1. ]\n",
      " [0.  0.  0.  ... 0.  0.  1. ]]\n"
     ]
    }
   ],
   "source": [
    "print(total.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['pao2ratio_vent', 'rate_dopamine', 'rate_epinephrine',\n",
      "       'rate_nonepinephrine', 'meabp_min', 'heartrate_max', 'temperature_max',\n",
      "       'gcs_min', 'bilirubin_max', 'creatineine_max', 'paltelet_min',\n",
      "       'respiration', 'coagulation', 'liver', 'cns', 'renal',\n",
      "       'respiration_24hours', 'coagulation_24hours', 'liver_24hours',\n",
      "       'cns_24hours', 'renal_24hours', 'sofa_24hours', 'is_infection'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "print(total.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xTdQQjnK5ZzF"
   },
   "source": [
    "# Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "d_zlRdiSTAgn"
   },
   "outputs": [],
   "source": [
    "class SepsisDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, df_data, seq_len=4):\n",
    "        self.seq_len = seq_len\n",
    "        self.X = df_data.loc[:,df_data.columns!='is_infection'].values\n",
    "        self.y = df_data.loc[:, 'is_infection'].values\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.X)//self.seq_len\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        idx += self.seq_len # 몇 시간 단위로 인덱스 올리는 거\n",
    "        X = self.X[idx-self.seq_len:idx-2]\n",
    "        y = self.y[idx-1]\n",
    "        return X, y "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1000, 2, 22]) torch.Size([1000])\n"
     ]
    }
   ],
   "source": [
    "train_dataset = SepsisDataset(total, seq_len=4)\n",
    "trainloader = DataLoader(train_dataset,  # dataset을 input으로\n",
    "                             batch_size=1000,\n",
    "                             shuffle=True, drop_last=True)\n",
    "\n",
    "for i, (X, y) in enumerate(trainloader) :\n",
    "    print(X.size() , y.size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Al40scAi5ZzG"
   },
   "source": [
    "# Model Define"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "Rr7F5kpl5ZzH"
   },
   "outputs": [],
   "source": [
    "class LSTM(nn.Module):\n",
    "    \n",
    "    def __init__(self, input_dim, hidden_dim, num_layers, batch_size, dropout, use_bn):\n",
    "        super(LSTM, self).__init__()\n",
    "        self.input_dim = input_dim \n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.output_dim = 1\n",
    "        self.num_layers = num_layers\n",
    "\n",
    "        self.batch_size = batch_size\n",
    "        self.dropout = dropout\n",
    "        self.use_bn = use_bn \n",
    "        \n",
    "        self.lstm = nn.LSTM(self.input_dim, self.hidden_dim, self.num_layers)\n",
    "        self.hidden = self.init_hidden()\n",
    "        self.regressor = self.make_regressor()\n",
    "        \n",
    "    def init_hidden(self):\n",
    "        return (torch.zeros(self.num_layers, self.batch_size, self.hidden_dim),\n",
    "                torch.zeros(self.num_layers, self.batch_size, self.hidden_dim))\n",
    "    \n",
    "    def make_regressor(self):\n",
    "        layers = []\n",
    "        if self.use_bn:\n",
    "            layers.append(nn.BatchNorm1d(self.hidden_dim))\n",
    "        layers.append(nn.Dropout(self.dropout))\n",
    "        \n",
    "        # 나중에 network(node 개수 바꿔 주는 옵션도 생각하기)\n",
    "        layers.append(nn.Linear(self.hidden_dim, self.hidden_dim // 2))\n",
    "        layers.append(nn.ReLU())\n",
    "        layers.append(nn.Linear(self.hidden_dim // 2, self.output_dim))\n",
    "        layers.append(nn.Sigmoid())\n",
    "        regressor = nn.Sequential(*layers)\n",
    "        return regressor\n",
    "    \n",
    "    def forward(self, x):\n",
    "      # Model Capacity 따라 y.sahpe이 달라짐\n",
    "        lstm_out, self.hidden = self.lstm(x, self.hidden)\n",
    "        y_pred = self.regressor(lstm_out[-1].view(self.batch_size, -1)) # 맨 마지막 h_t 갖고 예측 할거니까 -1\n",
    "#         t = Variable(torch.Tensor([0.5]))  # threshold\n",
    "#         out = (y_pred > t).float() * 1\n",
    "        return y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "pqir1JP_5ZzH"
   },
   "outputs": [],
   "source": [
    "def metric(y_pred, y_true):  \n",
    "#     auc = roc_auc_score(y_true, y_pred) \n",
    "    \n",
    "    perc_y_pred = y_pred.cpu().detach().numpy()\n",
    "    perc_y_true = y_true.cpu().detach().numpy()\n",
    "\n",
    "#     try:\n",
    "#         auc = roc_auc_score(perc_y_true, perc_y_pred)\n",
    "#     except ValueError:\n",
    "#         print('Error')\n",
    "#         auc = 0.99\n",
    "\n",
    "#     print('@@@@@@@@@@@@@')\n",
    "#     print(perc_y_pred)\n",
    "#     print(perc_y_true)\n",
    "    perc_y_pred = np.where(perc_y_pred > 0.5, 1, 0)\n",
    "    auc = accuracy_score(perc_y_true, perc_y_pred, normalize=True)\n",
    "#     print(auc)\n",
    "    return auc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0j20Sve95ZzH"
   },
   "source": [
    "# Train, Validate, Test "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "I7FqJ9EX5ZzI"
   },
   "outputs": [],
   "source": [
    "def train(model, partition, optimizer, loss_fn, args):\n",
    "  # batch 단위로 갖고 오는 것\n",
    "  # 무작위로 시작 record 갖고 옴\n",
    "  # 갖고 올때 batch size만큼 갖고 옴\n",
    "  # permutation, not random\n",
    "    trainloader = DataLoader(partition['train'],  # dataset을 input으로\n",
    "                             batch_size=args.batch_size,\n",
    "                             shuffle=True, drop_last=True)\n",
    "    model.train()\n",
    "    model.zero_grad()\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    train_acc = 0.0\n",
    "    train_loss = 0.0\n",
    "    for i, (X, y) in enumerate(trainloader):\n",
    "\n",
    "        # X : [10, n, 6] [n, 10, 6] 현재 타임스템프 안에\n",
    "        # Y : [10, m, 1] \n",
    "        X = X.transpose(0, 1).float().to(args.device)\n",
    "        y_true = y.float().to(args.device) \n",
    "\n",
    "        model.zero_grad()\n",
    "        optimizer.zero_grad()\n",
    "        model.hidden = [hidden.to(args.device) for hidden in model.init_hidden()]\n",
    "\n",
    "        y_pred = model(X)\n",
    "        loss = loss_fn(y_pred.view(-1), y_true.view(-1))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        train_loss += loss.item()\n",
    "        train_acc += metric(y_pred, y_true)\n",
    "\n",
    "    train_loss = train_loss / len(trainloader)\n",
    "    train_acc = train_acc / len(trainloader)\n",
    "    \n",
    "    return model, train_loss, train_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "JTX3hfCI5ZzI"
   },
   "outputs": [],
   "source": [
    "def validate(model, partition, loss_fn, args):\n",
    "    valloader = DataLoader(partition['val'], \n",
    "                           batch_size=args.batch_size, \n",
    "                           shuffle=False, drop_last=True)\n",
    "    model.eval()\n",
    "\n",
    "    val_acc = 0.0\n",
    "    val_loss = 0.0\n",
    "    with torch.no_grad():\n",
    "        for i, (X, y) in enumerate(valloader):\n",
    "\n",
    "            X = X.transpose(0, 1).float().to(args.device)\n",
    "            y_true = y.float().to(args.device)\n",
    "            model.hidden = [hidden.to(args.device) for hidden in model.init_hidden()]\n",
    "\n",
    "            y_pred = model(X)\n",
    "            loss = loss_fn(y_pred.view(-1), y_true.view(-1))\n",
    "\n",
    "            val_loss += loss.item()\n",
    "            val_acc += metric(y_pred, y_true)\n",
    "\n",
    "    val_loss = val_loss / len(valloader)\n",
    "    val_acc = val_acc / len(valloader)\n",
    "    return val_loss, val_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "kAmbRVca5ZzI"
   },
   "outputs": [],
   "source": [
    "def test(model, partition, args):\n",
    "    testloader = DataLoader(partition['test'], \n",
    "                           batch_size=args.batch_size, \n",
    "                           shuffle=False, drop_last=True)\n",
    "    model.eval()\n",
    "\n",
    "    test_acc = 0.0\n",
    "    cnt = 0\n",
    "    with torch.no_grad():\n",
    "        for i, (X, y) in enumerate(testloader):\n",
    "            cnt += 1\n",
    "#             print(cnt)\n",
    "            X = X.transpose(0, 1).float().to(args.device)\n",
    "            y_true = y.float().to(args.device)\n",
    "            model.hidden = [hidden.to(args.device) for hidden in model.init_hidden()]\n",
    "\n",
    "            y_pred = model(X)\n",
    "            if cnt == 1:\n",
    "                y_true_all = torch.tensor(y_true)\n",
    "                y_pred_all = torch.tensor(y_pred)\n",
    "            else:\n",
    "                y_true_all = torch.cat([y_true_all, y_true], dim=0)\n",
    "                y_pred_all = torch.cat([y_pred_all, y_pred], dim=0)\n",
    "                \n",
    "            test_acc += metric(y_pred, y_true)\n",
    "        \n",
    "    test_acc = test_acc / len(testloader)\n",
    "#     print(len(y_pred_all), len(y_true_all))\n",
    "    \n",
    "    return y_pred, test_acc, y_true_all, y_pred_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "RgQSze2O5ZzJ"
   },
   "outputs": [],
   "source": [
    "def experiment(partition, args):\n",
    "\n",
    "    model = LSTM(args.input_dim, args.hid_dim, args.n_layers, args.batch_size, args.dropout, args.use_bn)\n",
    "    model.to(args.device)\n",
    "\n",
    "    # loss_fn = torch.nn.BCELoss(weight=None, size_average=None, reduce=None, reduction='mean')\n",
    "    loss_fn = nn.BCELoss()\n",
    "    if args.optim == 'SGD':\n",
    "        optimizer = optim.RMSprop(model.parameters(), lr=args.lr, weight_decay=args.l2)\n",
    "    elif args.optim == 'RMSprop':\n",
    "        optimizer = optim.RMSprop(model.parameters(), lr=args.lr, weight_decay=args.l2)\n",
    "    elif args.optim == 'Adam':\n",
    "        optimizer = optim.Adam(model.parameters(), lr=args.lr, weight_decay=args.l2)\n",
    "    else:\n",
    "        raise ValueError('In-valid optimizer choice')\n",
    "    \n",
    "    # ===== List for epoch-wise data ====== #\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "    train_accs = []\n",
    "    val_accs = []\n",
    "    # ===================================== #\n",
    "    \n",
    "    for epoch in range(args.epoch):  # loop over the dataset multiple times\n",
    "        ts = time.time()\n",
    "        model, train_loss, train_acc = train(model, partition, optimizer, loss_fn, args)\n",
    "        val_loss, val_acc = validate(model, partition, loss_fn, args)\n",
    "        te = time.time()\n",
    "        \n",
    "        # ====== Add Epoch Data ====== #\n",
    "        train_losses.append(train_loss)\n",
    "        val_losses.append(val_loss)\n",
    "        train_accs.append(train_acc)\n",
    "        val_accs.append(val_acc)\n",
    "        # ============================ #\n",
    "        \n",
    "        if epoch % 10 == 0:\n",
    "            print('Epoch {}, Acc(train/val): {:2.2f}/{:2.2f}, Loss(train/val) {:2.5f}/{:2.5f}. Took {:2.2f} sec'.format(epoch, train_acc, val_acc, train_loss, val_loss, te-ts))\n",
    "        \n",
    "    y_pred, test_acc, y_true, y_pred_all = test(model, partition, args)   \n",
    "    print('len of temp:', len(temp))\n",
    "    \n",
    "    # ======= Add Result to Dictionary ======= #\n",
    "    result = {}\n",
    "    result['train_losses'] = train_losses\n",
    "    result['val_losses'] = val_losses\n",
    "    result['train_accs'] = train_accs\n",
    "    result['val_accs'] = val_accs\n",
    "    result['train_acc'] = train_acc\n",
    "    result['val_acc'] = val_acc\n",
    "    result['test_acc'] = test_acc\n",
    "#     result['pred'] = y_pred.cpu().detach().numpy()\n",
    "    return vars(args), result, y_true, y_pred_all"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7OUVge945ZzJ"
   },
   "source": [
    "# Manage Experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "6Ved9MOK5ZzJ"
   },
   "outputs": [],
   "source": [
    "import hashlib\n",
    "import json\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "import pandas as pd\n",
    "\n",
    "def save_exp_result(setting, result):\n",
    "    exp_name = setting['exp_name']\n",
    "    del setting['epoch']\n",
    "\n",
    "    hash_key = hashlib.sha1(str(setting).encode()).hexdigest()[:6]\n",
    "    filename = './results/{}-{}.json'.format(exp_name, hash_key)\n",
    "    result.update(setting)\n",
    "    with open(filename, 'w') as f:\n",
    "        json.dump(result, f)\n",
    "\n",
    "    \n",
    "def load_exp_result(exp_name):\n",
    "    dir_path = './results'\n",
    "    filenames = [f for f in listdir(dir_path) if isfile(join(dir_path, f)) if '.json' in f]\n",
    "    list_result = []\n",
    "    for filename in filenames:\n",
    "        if exp_name in filename:\n",
    "            with open(join(dir_path, filename), 'r') as infile:\n",
    "                results = json.load(infile)\n",
    "                list_result.append(results)\n",
    "    df = pd.DataFrame(list_result) # .drop(columns=[])\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "kfLW-ZBo5ZzJ"
   },
   "outputs": [],
   "source": [
    "\n",
    "def plot_acc(var1, var2, df):\n",
    "\n",
    "    fig, ax = plt.subplots(1, 3)\n",
    "    fig.set_size_inches(15, 6)\n",
    "    sns.set_style(\"darkgrid\", {\"axes.facecolor\": \".9\"})\n",
    "\n",
    "    sns.barplot(x=var1, y='train_acc', hue=var2, data=df, ax=ax[0])\n",
    "    sns.barplot(x=var1, y='val_acc', hue=var2, data=df, ax=ax[1])\n",
    "    sns.barplot(x=var1, y='test_acc', hue=var2, data=df, ax=ax[2])\n",
    "    \n",
    "    ax[0].set_title('Train Accuracy')\n",
    "    ax[1].set_title('Validation Accuracy')\n",
    "    ax[2].set_title('Test Accuracy')\n",
    "\n",
    "    \n",
    "def plot_loss_variation(var1, var2, df, **kwargs):\n",
    "\n",
    "    list_v1 = df[var1].unique()\n",
    "    list_v2 = df[var2].unique()\n",
    "    list_data = []\n",
    "\n",
    "    for value1 in list_v1:\n",
    "        for value2 in list_v2:\n",
    "            row = df.loc[df[var1]==value1]\n",
    "            row = row.loc[df[var2]==value2]\n",
    "\n",
    "            train_losses = list(row.train_losses)[0]\n",
    "            val_losses = list(row.val_losses)[0]\n",
    "\n",
    "            for epoch, train_loss in enumerate(train_losses):\n",
    "                list_data.append({'type':'train', 'loss':train_loss, 'epoch':epoch, var1:value1, var2:value2})\n",
    "            for epoch, val_loss in enumerate(val_losses):\n",
    "                list_data.append({'type':'val', 'loss':val_loss, 'epoch':epoch, var1:value1, var2:value2})\n",
    "\n",
    "    df = pd.DataFrame(list_data)\n",
    "    g = sns.FacetGrid(df, row=var2, col=var1, hue='type', **kwargs)\n",
    "    g = g.map(plt.plot, 'epoch', 'loss', marker='.')\n",
    "    g.add_legend()\n",
    "    g.fig.suptitle('Train loss vs Val loss')\n",
    "    plt.subplots_adjust(top=0.89) # 만약 Title이 그래프랑 겹친다면 top 값을 조정해주면 됩니다! 함수 인자로 받으면 그래프마다 조절할 수 있겠죠?\n",
    "\n",
    "\n",
    "def plot_acc_variation(var1, var2, df, **kwargs):\n",
    "    list_v1 = df[var1].unique()\n",
    "    list_v2 = df[var2].unique()\n",
    "    list_data = []\n",
    "\n",
    "    for value1 in list_v1:\n",
    "        for value2 in list_v2:\n",
    "            row = df.loc[df[var1]==value1]\n",
    "            row = row.loc[df[var2]==value2]\n",
    "\n",
    "            train_accs = list(row.train_accs)[0]\n",
    "            val_accs = list(row.val_accs)[0]\n",
    "            test_acc = list(row.test_acc)[0]\n",
    "\n",
    "            for epoch, train_acc in enumerate(train_accs):\n",
    "                list_data.append({'type':'train', 'Acc':train_acc, 'test_acc':test_acc, 'epoch':epoch, var1:value1, var2:value2})\n",
    "            for epoch, val_acc in enumerate(val_accs):\n",
    "                list_data.append({'type':'val', 'Acc':val_acc, 'test_acc':test_acc, 'epoch':epoch, var1:value1, var2:value2})\n",
    "\n",
    "    df = pd.DataFrame(list_data)\n",
    "    g = sns.FacetGrid(df, row=var2, col=var1, hue='type', **kwargs)\n",
    "    g = g.map(plt.plot, 'epoch', 'Acc', marker='.')\n",
    "\n",
    "    def show_acc(x, y, metric, **kwargs):\n",
    "        plt.scatter(x, y, alpha=0.3, s=1)\n",
    "        metric = \"Test Acc: {:1.3f}\".format(list(metric.values)[0])\n",
    "        plt.text(0.05, 0.95, metric,  horizontalalignment='left', verticalalignment='center', transform=plt.gca().transAxes, bbox=dict(facecolor='yellow', alpha=0.5, boxstyle=\"round,pad=0.1\"))\n",
    "    g = g.map(show_acc, 'epoch', 'Acc', 'test_acc')\n",
    "\n",
    "    g.add_legend()\n",
    "    g.fig.suptitle('Train Accuracy vs Val Accuracy')\n",
    "    plt.subplots_adjust(top=0.89)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "260.5\n"
     ]
    }
   ],
   "source": [
    "train_val_dataset = total[:int((len(total) / 4)*.8)*4]\n",
    "test_dataset = total[int((len(total) / 4)*.8)*4:]\n",
    "train_dataset = train_val_dataset[:int((len(train_val_dataset) / 4)*.8)*4]\n",
    "valid_dataset = train_val_dataset[int((len(train_val_dataset) / 4)*.8)*4:]\n",
    "\n",
    "trainset = SepsisDataset(train_dataset, seq_len=4)\n",
    "valset = SepsisDataset(valid_dataset, seq_len=4)\n",
    "testset = SepsisDataset(test_dataset, seq_len=4)\n",
    "\n",
    "print(len(test_dataset)/4)\n",
    "\n",
    "partition = {'train': trainset, 'val':valset, 'test':testset}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 375
    },
    "id": "D2umJYQT5ZzK",
    "outputId": "96e326e4-74a7-4428-b5f5-213e83ae931c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Namespace(batch_size=64, device='cuda', dropout=0.0, epoch=50, exp_name='exp1_lr', hid_dim=50, input_dim=22, l2=1e-05, lr=0.001, n_layers=1, optim='Adam', use_bn=True)\n",
      "Epoch 0, Acc(train/val): 0.79/0.96, Loss(train/val) 0.62128/0.61812. Took 0.08 sec\n",
      "Epoch 10, Acc(train/val): 0.96/0.96, Loss(train/val) 0.15288/0.15770. Took 0.05 sec\n",
      "Epoch 20, Acc(train/val): 0.96/0.96, Loss(train/val) 0.11822/0.15890. Took 0.07 sec\n",
      "Epoch 30, Acc(train/val): 0.97/0.97, Loss(train/val) 0.09397/0.17362. Took 0.08 sec\n",
      "Epoch 40, Acc(train/val): 0.97/0.95, Loss(train/val) 0.07898/0.20987. Took 0.06 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:19: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:20: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len of temp: 201\n",
      "Namespace(batch_size=64, device='cuda', dropout=0.0, epoch=50, exp_name='exp1_lr', hid_dim=50, input_dim=22, l2=1e-05, lr=0.001, n_layers=2, optim='Adam', use_bn=True)\n",
      "Epoch 0, Acc(train/val): 0.86/0.22, Loss(train/val) 0.56533/0.69596. Took 0.11 sec\n",
      "Epoch 10, Acc(train/val): 0.96/0.96, Loss(train/val) 0.13799/0.17580. Took 0.11 sec\n",
      "Epoch 20, Acc(train/val): 0.96/0.94, Loss(train/val) 0.10388/0.20437. Took 0.09 sec\n",
      "Epoch 30, Acc(train/val): 0.97/0.92, Loss(train/val) 0.08066/0.22249. Took 0.07 sec\n",
      "Epoch 40, Acc(train/val): 0.97/0.90, Loss(train/val) 0.07145/0.27980. Took 0.08 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:19: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:20: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len of temp: 201\n",
      "Namespace(batch_size=64, device='cuda', dropout=0.0, epoch=50, exp_name='exp1_lr', hid_dim=50, input_dim=22, l2=1e-05, lr=0.001, n_layers=3, optim='Adam', use_bn=True)\n",
      "Epoch 0, Acc(train/val): 0.96/0.96, Loss(train/val) 0.50656/0.59160. Took 0.09 sec\n",
      "Epoch 10, Acc(train/val): 0.96/0.96, Loss(train/val) 0.13366/0.16047. Took 0.11 sec\n",
      "Epoch 20, Acc(train/val): 0.96/0.95, Loss(train/val) 0.10757/0.17954. Took 0.11 sec\n",
      "Epoch 30, Acc(train/val): 0.97/0.94, Loss(train/val) 0.09272/0.20273. Took 0.07 sec\n",
      "Epoch 40, Acc(train/val): 0.97/0.92, Loss(train/val) 0.08136/0.24205. Took 0.09 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:19: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:20: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len of temp: 201\n",
      "Namespace(batch_size=64, device='cuda', dropout=0.0, epoch=50, exp_name='exp1_lr', hid_dim=50, input_dim=22, l2=1e-05, lr=0.0001, n_layers=1, optim='Adam', use_bn=True)\n",
      "Epoch 0, Acc(train/val): 0.86/0.96, Loss(train/val) 0.61303/0.63602. Took 0.05 sec\n",
      "Epoch 10, Acc(train/val): 0.96/0.96, Loss(train/val) 0.46358/0.46034. Took 0.06 sec\n",
      "Epoch 20, Acc(train/val): 0.96/0.96, Loss(train/val) 0.35165/0.35702. Took 0.05 sec\n",
      "Epoch 30, Acc(train/val): 0.96/0.96, Loss(train/val) 0.27482/0.28241. Took 0.06 sec\n",
      "Epoch 40, Acc(train/val): 0.96/0.96, Loss(train/val) 0.22689/0.23583. Took 0.06 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:19: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:20: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len of temp: 201\n",
      "Namespace(batch_size=64, device='cuda', dropout=0.0, epoch=50, exp_name='exp1_lr', hid_dim=50, input_dim=22, l2=1e-05, lr=0.0001, n_layers=2, optim='Adam', use_bn=True)\n",
      "Epoch 0, Acc(train/val): 0.90/0.96, Loss(train/val) 0.60489/0.62110. Took 0.07 sec\n",
      "Epoch 10, Acc(train/val): 0.96/0.96, Loss(train/val) 0.43035/0.42277. Took 0.07 sec\n",
      "Epoch 20, Acc(train/val): 0.96/0.96, Loss(train/val) 0.30546/0.31864. Took 0.10 sec\n",
      "Epoch 30, Acc(train/val): 0.96/0.96, Loss(train/val) 0.23544/0.25749. Took 0.07 sec\n",
      "Epoch 40, Acc(train/val): 0.96/0.96, Loss(train/val) 0.19699/0.22459. Took 0.06 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:19: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:20: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len of temp: 201\n",
      "Namespace(batch_size=64, device='cuda', dropout=0.0, epoch=50, exp_name='exp1_lr', hid_dim=50, input_dim=22, l2=1e-05, lr=0.0001, n_layers=3, optim='Adam', use_bn=True)\n",
      "Epoch 0, Acc(train/val): 0.04/0.04, Loss(train/val) 0.83241/0.79526. Took 0.07 sec\n",
      "Epoch 10, Acc(train/val): 0.68/0.66, Loss(train/val) 0.59688/0.54847. Took 0.07 sec\n",
      "Epoch 20, Acc(train/val): 0.91/0.91, Loss(train/val) 0.41309/0.41065. Took 0.09 sec\n",
      "Epoch 30, Acc(train/val): 0.94/0.94, Loss(train/val) 0.31090/0.32937. Took 0.07 sec\n",
      "Epoch 40, Acc(train/val): 0.95/0.95, Loss(train/val) 0.24552/0.27335. Took 0.08 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:19: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:20: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len of temp: 201\n",
      "Namespace(batch_size=64, device='cuda', dropout=0.0, epoch=50, exp_name='exp1_lr', hid_dim=50, input_dim=22, l2=1e-05, lr=1e-05, n_layers=1, optim='Adam', use_bn=True)\n",
      "Epoch 0, Acc(train/val): 0.28/0.70, Loss(train/val) 0.75813/0.68736. Took 0.07 sec\n",
      "Epoch 10, Acc(train/val): 0.33/0.24, Loss(train/val) 0.73963/0.76276. Took 0.06 sec\n",
      "Epoch 20, Acc(train/val): 0.37/0.30, Loss(train/val) 0.72080/0.74297. Took 0.06 sec\n",
      "Epoch 30, Acc(train/val): 0.43/0.32, Loss(train/val) 0.70352/0.72576. Took 0.06 sec\n",
      "Epoch 40, Acc(train/val): 0.49/0.35, Loss(train/val) 0.68750/0.70709. Took 0.06 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:19: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:20: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len of temp: 201\n",
      "Namespace(batch_size=64, device='cuda', dropout=0.0, epoch=50, exp_name='exp1_lr', hid_dim=50, input_dim=22, l2=1e-05, lr=1e-05, n_layers=2, optim='Adam', use_bn=True)\n",
      "Epoch 0, Acc(train/val): 0.07/0.04, Loss(train/val) 0.84613/0.78050. Took 0.06 sec\n",
      "Epoch 10, Acc(train/val): 0.11/0.06, Loss(train/val) 0.81500/0.83266. Took 0.06 sec\n",
      "Epoch 20, Acc(train/val): 0.15/0.10, Loss(train/val) 0.78541/0.80303. Took 0.06 sec\n",
      "Epoch 30, Acc(train/val): 0.23/0.16, Loss(train/val) 0.75879/0.77511. Took 0.06 sec\n",
      "Epoch 40, Acc(train/val): 0.30/0.26, Loss(train/val) 0.73350/0.74968. Took 0.06 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:19: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:20: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len of temp: 201\n",
      "Namespace(batch_size=64, device='cuda', dropout=0.0, epoch=50, exp_name='exp1_lr', hid_dim=50, input_dim=22, l2=1e-05, lr=1e-05, n_layers=3, optim='Adam', use_bn=True)\n",
      "Epoch 0, Acc(train/val): 0.81/0.96, Loss(train/val) 0.65807/0.67187. Took 0.07 sec\n",
      "Epoch 10, Acc(train/val): 0.91/0.89, Loss(train/val) 0.64098/0.64474. Took 0.07 sec\n",
      "Epoch 20, Acc(train/val): 0.94/0.95, Loss(train/val) 0.62292/0.62512. Took 0.08 sec\n"
     ]
    }
   ],
   "source": [
    "# ====== Random Seed Initialization ====== #\n",
    "seed = 666\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "\n",
    "parser = argparse.ArgumentParser()\n",
    "args = parser.parse_args(\"\")\n",
    "args.exp_name = \"exp1_lr\"\n",
    "args.device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "args.batch_size = 64\n",
    "\n",
    "# ====== Model Capacity ===== #\n",
    "args.input_dim = 22\n",
    "args.hid_dim = 50\n",
    "args.n_layers = 2\n",
    "\n",
    "# ====== Regularization ======= #\n",
    "args.l2 = 0.00001\n",
    "args.dropout = 0.0\n",
    "args.use_bn = True\n",
    "\n",
    "# ====== Optimizer & Training ====== #\n",
    "args.optim = 'Adam' #'RMSprop' #SGD, RMSprop, ADAM...\n",
    "args.lr = 0.0001\n",
    "args.epoch = 50\n",
    "\n",
    "\n",
    "# ====== Experiment Variable ====== #\n",
    "name_var1 = 'lr'\n",
    "name_var2 = 'n_layers'\n",
    "list_var1 = [0.001, 0.0001, 0.00001]\n",
    "list_var2 = [1,2,3]\n",
    "\n",
    "for var1 in list_var1:\n",
    "    for var2 in list_var2:\n",
    "        setattr(args, name_var1, var1)\n",
    "        setattr(args, name_var2, var2)\n",
    "        print(args)\n",
    "                \n",
    "        setting, result, y_true, y_pred_all = experiment(partition, deepcopy(args))\n",
    "        save_exp_result(setting, result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "N-lGOJcT5ZzK"
   },
   "outputs": [],
   "source": [
    "var1 = 'lr'\n",
    "var2 = 'n_layers'\n",
    "df = load_exp_result('exp1')\n",
    "\n",
    "plot_acc(var1, var2, df)\n",
    "plot_loss_variation(var1, var2, df, sharey=False) #sharey를 True로 하면 모둔 subplot의 y축의 스케일이 같아집니다.\n",
    "plot_acc_variation(var1, var2, df, margin_titles=True, sharey=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_all = y_pred_all.cpu().detach().numpy()\n",
    "y_true = y_true.cpu().detach().numpy()\n",
    "y_pred_all = np.where(y_pred_all > 0.5, 1, 0)\n",
    "\n",
    "print(len(y_pred_all))\n",
    "print(len(y_true))\n",
    "\n",
    "auc = accuracy_score(y_true, y_pred_all, normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "auc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "t = classification_report(y_true, y_pred_all, target_names=['0', '1'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(type(t))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_all.sum()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_true.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 혼동행렬, 정확도, 정밀도, 재현율, F1, AUC 불러오기\n",
    "def get_clf_eval(y_test, y_pred):\n",
    "    confusion = confusion_matrix(y_test, y_pred)\n",
    "    tt = confusion[0,0]\n",
    "    tf = confusion[0,1]\n",
    "    ft = confusion[1,0]\n",
    "    ff = confusion[1,1]\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    precision = precision_score(y_test, y_pred,average='weighted') # None, 'micro', 'macro', 'weighted'\n",
    "    recall = recall_score(y_test, y_pred,average='weighted')\n",
    "    F1 = f1_score(y_test, y_pred,average='weighted')\n",
    "    print('오차행렬:\\n', confusion)\n",
    "    print('\\n정확도: {:.4f}'.format(accuracy))\n",
    "    print('정밀도: {:.4f}'.format(precision))\n",
    "    print('재현율: {:.4f}'.format(recall))    \n",
    "    print('양성예측율: {:.4f}'.format(tt/(tt+tf)))\n",
    "    print('음성예측율: {:.4f}'.format(ff/(ft+ff)))    \n",
    "    print('F1: {:.4f}'.format(F1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_clf_eval(y_true, y_pred_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Sepsis_with_LSTM.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
