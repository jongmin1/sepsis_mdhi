{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Gqlpt6bO5ZzD",
    "outputId": "04886b08-66a6-4076-c8a9-7d78da19622b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.9.0+cu111\n",
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/IPython/core/magics/pylab.py:160: UserWarning: pylab import has clobbered these variables: ['seed', 'partition', 'test']\n",
      "`%matplotlib` prevents importing * from pylab and numpy\n",
      "  \"\\n`%matplotlib` prevents importing * from pylab and numpy\"\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import datetime\n",
    "import torch\n",
    "from torch.autograd import Variable\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import argparse\n",
    "from copy import deepcopy\n",
    "import seaborn as sns \n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score, mean_absolute_error\n",
    "from sklearn.preprocessing import binarize\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, precision_score, recall_score,f1_score ,roc_auc_score, explained_variance_score, mean_squared_error, r2_score, plot_roc_curve\n",
    "\n",
    "print(torch.__version__)\n",
    "%matplotlib inline\n",
    "%pylab inline\n",
    "pylab.rcParams['figure.figsize'] = (15, 9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "__DRQb_p5ZzE"
   },
   "source": [
    "# Pandas Datareader Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "214104\n"
     ]
    }
   ],
   "source": [
    "total = pd.read_csv('./result/qsofa_4h.csv')\n",
    "print(len(total))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(len(total))\n",
    "# temp = total[total['is_infection']==1]\n",
    "# total = pd.concat([total, temp])\n",
    "# total = pd.concat([total, temp])\n",
    "# e = len(total[total['is_infection']==1])\n",
    "# print(e*4/len(total))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['starttime', 'stay_id', 'hr', 'endtime', 'sbp', 'gcs_min',\n",
       "       'respiratory_rate', 'bp_score', 'rr_score', 'gcs_score', 'qsofa',\n",
       "       'event'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 193,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['starttime', 'stay_id', 'hr', 'endtime', 'sbp', 'gcs_min',\n",
       "       'respiratory_rate', 'bp_score', 'rr_score', 'gcs_score', 'qsofa',\n",
       "       'event'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 194,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total.columns\n",
    "# print(type(total))\n",
    "total['starttime'] = pd.to_datetime(total['starttime'])\n",
    "total.replace({'event': True}, {'event': 1}, inplace=True)\n",
    "total.replace({'event': False}, {'event': 0}, inplace=True)\n",
    "total.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>starttime</th>\n",
       "      <th>stay_id</th>\n",
       "      <th>hr</th>\n",
       "      <th>endtime</th>\n",
       "      <th>sbp</th>\n",
       "      <th>gcs_min</th>\n",
       "      <th>respiratory_rate</th>\n",
       "      <th>bp_score</th>\n",
       "      <th>rr_score</th>\n",
       "      <th>gcs_score</th>\n",
       "      <th>qsofa</th>\n",
       "      <th>event</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2130-05-25 13:00:00</td>\n",
       "      <td>35258336</td>\n",
       "      <td>0</td>\n",
       "      <td>2130-05-25 17:00:00</td>\n",
       "      <td>100.025000</td>\n",
       "      <td>15.0</td>\n",
       "      <td>15.941667</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2130-05-25 17:00:00</td>\n",
       "      <td>35258336</td>\n",
       "      <td>1</td>\n",
       "      <td>2130-05-25 21:00:00</td>\n",
       "      <td>108.875000</td>\n",
       "      <td>15.0</td>\n",
       "      <td>15.750000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2130-05-25 21:00:00</td>\n",
       "      <td>35258336</td>\n",
       "      <td>2</td>\n",
       "      <td>2130-05-26 01:00:00</td>\n",
       "      <td>108.250000</td>\n",
       "      <td>15.0</td>\n",
       "      <td>12.750000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2130-05-26 01:00:00</td>\n",
       "      <td>35258336</td>\n",
       "      <td>3</td>\n",
       "      <td>2130-05-26 05:00:00</td>\n",
       "      <td>102.750000</td>\n",
       "      <td>15.0</td>\n",
       "      <td>12.750000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2167-08-09 05:00:00</td>\n",
       "      <td>35938282</td>\n",
       "      <td>0</td>\n",
       "      <td>2167-08-09 09:00:00</td>\n",
       "      <td>111.666667</td>\n",
       "      <td>15.0</td>\n",
       "      <td>23.250000</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2167-08-09 09:00:00</td>\n",
       "      <td>35938282</td>\n",
       "      <td>1</td>\n",
       "      <td>2167-08-09 13:00:00</td>\n",
       "      <td>108.250000</td>\n",
       "      <td>15.0</td>\n",
       "      <td>21.250000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>2167-08-09 13:00:00</td>\n",
       "      <td>35938282</td>\n",
       "      <td>2</td>\n",
       "      <td>2167-08-09 17:00:00</td>\n",
       "      <td>114.166667</td>\n",
       "      <td>15.0</td>\n",
       "      <td>22.250000</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>2167-08-09 17:00:00</td>\n",
       "      <td>35938282</td>\n",
       "      <td>3</td>\n",
       "      <td>2167-08-09 21:00:00</td>\n",
       "      <td>120.500000</td>\n",
       "      <td>15.0</td>\n",
       "      <td>17.166667</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>2136-10-03 18:00:00</td>\n",
       "      <td>34582336</td>\n",
       "      <td>0</td>\n",
       "      <td>2136-10-03 22:00:00</td>\n",
       "      <td>162.375000</td>\n",
       "      <td>15.0</td>\n",
       "      <td>14.166667</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>2136-10-03 22:00:00</td>\n",
       "      <td>34582336</td>\n",
       "      <td>1</td>\n",
       "      <td>2136-10-04 02:00:00</td>\n",
       "      <td>160.250000</td>\n",
       "      <td>15.0</td>\n",
       "      <td>16.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            starttime   stay_id  hr              endtime         sbp  gcs_min  \\\n",
       "0 2130-05-25 13:00:00  35258336   0  2130-05-25 17:00:00  100.025000     15.0   \n",
       "1 2130-05-25 17:00:00  35258336   1  2130-05-25 21:00:00  108.875000     15.0   \n",
       "2 2130-05-25 21:00:00  35258336   2  2130-05-26 01:00:00  108.250000     15.0   \n",
       "3 2130-05-26 01:00:00  35258336   3  2130-05-26 05:00:00  102.750000     15.0   \n",
       "4 2167-08-09 05:00:00  35938282   0  2167-08-09 09:00:00  111.666667     15.0   \n",
       "5 2167-08-09 09:00:00  35938282   1  2167-08-09 13:00:00  108.250000     15.0   \n",
       "6 2167-08-09 13:00:00  35938282   2  2167-08-09 17:00:00  114.166667     15.0   \n",
       "7 2167-08-09 17:00:00  35938282   3  2167-08-09 21:00:00  120.500000     15.0   \n",
       "8 2136-10-03 18:00:00  34582336   0  2136-10-03 22:00:00  162.375000     15.0   \n",
       "9 2136-10-03 22:00:00  34582336   1  2136-10-04 02:00:00  160.250000     15.0   \n",
       "\n",
       "   respiratory_rate  bp_score  rr_score  gcs_score  qsofa  event  \n",
       "0         15.941667         0         0          0      0    0.0  \n",
       "1         15.750000         0         0          0      0    0.0  \n",
       "2         12.750000         0         0          0      0    0.0  \n",
       "3         12.750000         0         0          0      0    0.0  \n",
       "4         23.250000         0         1          0      1    0.0  \n",
       "5         21.250000         0         0          0      0    0.0  \n",
       "6         22.250000         0         1          0      1    0.0  \n",
       "7         17.166667         0         0          0      0    0.0  \n",
       "8         14.166667         0         0          0      0    0.0  \n",
       "9         16.000000         0         0          0      0    0.0  "
      ]
     },
     "execution_count": 195,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [],
   "source": [
    "total=total.set_index('starttime')\n",
    "# total=total.sort_values(by=['Unnamed: 0'])\n",
    "total = total.drop(['stay_id', 'hr', 'endtime', 'bp_score', 'rr_score', 'gcs_score', 'qsofa'], axis=1)\n",
    "#total = total.astype('float')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatetimeIndex(['2130-05-25 13:00:00', '2130-05-25 17:00:00',\n",
      "               '2130-05-25 21:00:00', '2130-05-26 01:00:00',\n",
      "               '2167-08-09 05:00:00', '2167-08-09 09:00:00',\n",
      "               '2167-08-09 13:00:00', '2167-08-09 17:00:00',\n",
      "               '2136-10-03 18:00:00', '2136-10-03 22:00:00',\n",
      "               ...\n",
      "               '2176-06-15 22:00:00', '2176-06-16 02:00:00',\n",
      "               '2139-05-26 18:00:00', '2139-05-26 22:00:00',\n",
      "               '2139-05-27 02:00:00', '2139-05-27 06:00:00',\n",
      "               '2164-03-22 17:00:00', '2164-03-22 21:00:00',\n",
      "               '2164-03-23 01:00:00', '2164-03-23 05:00:00'],\n",
      "              dtype='datetime64[ns]', name='starttime', length=214104, freq=None)\n"
     ]
    }
   ],
   "source": [
    "total.columns\n",
    "print(total.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[100.025       15.          15.94166667   0.        ]\n",
      " [108.875       15.          15.75         0.        ]\n",
      " [108.25        15.          12.75         0.        ]\n",
      " ...\n",
      " [107.5         15.          25.75         0.        ]\n",
      " [106.75        15.          26.5          0.        ]\n",
      " [105.75        15.          25.           0.        ]]\n"
     ]
    }
   ],
   "source": [
    "print(total.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['sbp', 'gcs_min', 'respiratory_rate', 'event'], dtype='object')\n"
     ]
    }
   ],
   "source": [
    "print(total.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xTdQQjnK5ZzF"
   },
   "source": [
    "# Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {
    "id": "d_zlRdiSTAgn"
   },
   "outputs": [],
   "source": [
    "class SepsisDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, df_data, seq_len=4):\n",
    "        self.seq_len = seq_len\n",
    "        self.X = df_data.loc[:,df_data.columns!='event'].values\n",
    "        self.y = df_data.loc[:, 'event'].values\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.X)//self.seq_len\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        idx += self.seq_len # 몇 시간 단위로 인덱스 올리는 거\n",
    "        X = self.X[idx-self.seq_len:idx-2]\n",
    "        y = self.y[idx-1]\n",
    "        return X, y "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1000, 2, 3]) torch.Size([1000])\n",
      "torch.Size([1000, 2, 3]) torch.Size([1000])\n",
      "torch.Size([1000, 2, 3]) torch.Size([1000])\n",
      "torch.Size([1000, 2, 3]) torch.Size([1000])\n",
      "torch.Size([1000, 2, 3]) torch.Size([1000])\n",
      "torch.Size([1000, 2, 3]) torch.Size([1000])\n",
      "torch.Size([1000, 2, 3]) torch.Size([1000])\n",
      "torch.Size([1000, 2, 3]) torch.Size([1000])\n",
      "torch.Size([1000, 2, 3]) torch.Size([1000])\n",
      "torch.Size([1000, 2, 3]) torch.Size([1000])\n",
      "torch.Size([1000, 2, 3]) torch.Size([1000])\n",
      "torch.Size([1000, 2, 3]) torch.Size([1000])\n",
      "torch.Size([1000, 2, 3]) torch.Size([1000])\n",
      "torch.Size([1000, 2, 3]) torch.Size([1000])\n",
      "torch.Size([1000, 2, 3]) torch.Size([1000])\n",
      "torch.Size([1000, 2, 3]) torch.Size([1000])\n",
      "torch.Size([1000, 2, 3]) torch.Size([1000])\n",
      "torch.Size([1000, 2, 3]) torch.Size([1000])\n",
      "torch.Size([1000, 2, 3]) torch.Size([1000])\n",
      "torch.Size([1000, 2, 3]) torch.Size([1000])\n",
      "torch.Size([1000, 2, 3]) torch.Size([1000])\n",
      "torch.Size([1000, 2, 3]) torch.Size([1000])\n",
      "torch.Size([1000, 2, 3]) torch.Size([1000])\n",
      "torch.Size([1000, 2, 3]) torch.Size([1000])\n",
      "torch.Size([1000, 2, 3]) torch.Size([1000])\n",
      "torch.Size([1000, 2, 3]) torch.Size([1000])\n",
      "torch.Size([1000, 2, 3]) torch.Size([1000])\n",
      "torch.Size([1000, 2, 3]) torch.Size([1000])\n",
      "torch.Size([1000, 2, 3]) torch.Size([1000])\n",
      "torch.Size([1000, 2, 3]) torch.Size([1000])\n",
      "torch.Size([1000, 2, 3]) torch.Size([1000])\n",
      "torch.Size([1000, 2, 3]) torch.Size([1000])\n",
      "torch.Size([1000, 2, 3]) torch.Size([1000])\n",
      "torch.Size([1000, 2, 3]) torch.Size([1000])\n",
      "torch.Size([1000, 2, 3]) torch.Size([1000])\n",
      "torch.Size([1000, 2, 3]) torch.Size([1000])\n",
      "torch.Size([1000, 2, 3]) torch.Size([1000])\n",
      "torch.Size([1000, 2, 3]) torch.Size([1000])\n",
      "torch.Size([1000, 2, 3]) torch.Size([1000])\n",
      "torch.Size([1000, 2, 3]) torch.Size([1000])\n",
      "torch.Size([1000, 2, 3]) torch.Size([1000])\n",
      "torch.Size([1000, 2, 3]) torch.Size([1000])\n",
      "torch.Size([1000, 2, 3]) torch.Size([1000])\n",
      "torch.Size([1000, 2, 3]) torch.Size([1000])\n",
      "torch.Size([1000, 2, 3]) torch.Size([1000])\n",
      "torch.Size([1000, 2, 3]) torch.Size([1000])\n",
      "torch.Size([1000, 2, 3]) torch.Size([1000])\n",
      "torch.Size([1000, 2, 3]) torch.Size([1000])\n",
      "torch.Size([1000, 2, 3]) torch.Size([1000])\n",
      "torch.Size([1000, 2, 3]) torch.Size([1000])\n",
      "torch.Size([1000, 2, 3]) torch.Size([1000])\n",
      "torch.Size([1000, 2, 3]) torch.Size([1000])\n",
      "torch.Size([1000, 2, 3]) torch.Size([1000])\n"
     ]
    }
   ],
   "source": [
    "train_dataset = SepsisDataset(total, seq_len=4)\n",
    "trainloader = DataLoader(train_dataset,  # dataset을 input으로\n",
    "                             batch_size=1000,\n",
    "                             shuffle=True, drop_last=True)\n",
    "\n",
    "for i, (X, y) in enumerate(trainloader) :\n",
    "    print(X.size() , y.size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Al40scAi5ZzG"
   },
   "source": [
    "# Model Define"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {
    "id": "Rr7F5kpl5ZzH"
   },
   "outputs": [],
   "source": [
    "class LSTM(nn.Module):\n",
    "    \n",
    "    def __init__(self, input_dim, hidden_dim, num_layers, batch_size, dropout, use_bn):\n",
    "        super(LSTM, self).__init__()\n",
    "        self.input_dim = input_dim \n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.output_dim = 1\n",
    "        self.num_layers = num_layers\n",
    "\n",
    "        self.batch_size = batch_size\n",
    "        self.dropout = dropout\n",
    "        self.use_bn = use_bn \n",
    "        \n",
    "        self.lstm = nn.LSTM(self.input_dim, self.hidden_dim, self.num_layers)\n",
    "        self.hidden = self.init_hidden()\n",
    "        self.regressor = self.make_regressor()\n",
    "        \n",
    "    def init_hidden(self):\n",
    "        return (torch.zeros(self.num_layers, self.batch_size, self.hidden_dim),\n",
    "                torch.zeros(self.num_layers, self.batch_size, self.hidden_dim))\n",
    "    \n",
    "    def make_regressor(self):\n",
    "        layers = []\n",
    "        if self.use_bn:\n",
    "            layers.append(nn.BatchNorm1d(self.hidden_dim))\n",
    "        layers.append(nn.Dropout(self.dropout))\n",
    "        \n",
    "        # 나중에 network(node 개수 바꿔 주는 옵션도 생각하기)\n",
    "        layers.append(nn.Linear(self.hidden_dim, self.hidden_dim // 2))\n",
    "        layers.append(nn.ReLU())\n",
    "        layers.append(nn.Linear(self.hidden_dim // 2, self.output_dim))\n",
    "        layers.append(nn.Sigmoid())\n",
    "        regressor = nn.Sequential(*layers)\n",
    "        return regressor\n",
    "    \n",
    "    def forward(self, x):\n",
    "      # Model Capacity 따라 y.sahpe이 달라짐\n",
    "        lstm_out, self.hidden = self.lstm(x, self.hidden)\n",
    "        y_pred = self.regressor(lstm_out[-1].view(self.batch_size, -1)) # 맨 마지막 h_t 갖고 예측 할거니까 -1\n",
    "#         t = Variable(torch.Tensor([0.5]))  # threshold\n",
    "#         out = (y_pred > t).float() * 1\n",
    "        return y_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0j20Sve95ZzH"
   },
   "source": [
    "# Train, Validate, Test "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {
    "id": "I7FqJ9EX5ZzI"
   },
   "outputs": [],
   "source": [
    "def train(model, partition, optimizer, loss_fn, args):\n",
    "  # batch 단위로 갖고 오는 것\n",
    "  # 무작위로 시작 record 갖고 옴\n",
    "  # 갖고 올때 batch size만큼 갖고 옴\n",
    "  # permutation, not random\n",
    "    trainloader = DataLoader(partition['train'],  # dataset을 input으로\n",
    "                             batch_size=args.batch_size,\n",
    "                             shuffle=True, drop_last=True)\n",
    "    model.train()\n",
    "    model.zero_grad()\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    train_acc = 0.0\n",
    "    train_loss = 0.0\n",
    "    for i, (X, y) in enumerate(trainloader):\n",
    "\n",
    "        # X : [10, n, 6] [n, 10, 6] 현재 타임스템프 안에\n",
    "        # Y : [10, m, 1] \n",
    "        X = X.transpose(0, 1).float().to(args.device)\n",
    "        y_true = y.float().to(args.device) \n",
    "\n",
    "        model.zero_grad()\n",
    "        optimizer.zero_grad()\n",
    "        model.hidden = [hidden.to(args.device) for hidden in model.init_hidden()]\n",
    "\n",
    "        y_pred = model(X)\n",
    "        loss = loss_fn(y_pred.view(-1), y_true.view(-1))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        train_loss += loss.item()\n",
    "        train_acc += metric(y_pred, y_true)\n",
    "\n",
    "    train_loss = train_loss / len(trainloader)\n",
    "    train_acc = train_acc / len(trainloader)\n",
    "    \n",
    "    return model, train_loss, train_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {
    "id": "JTX3hfCI5ZzI"
   },
   "outputs": [],
   "source": [
    "def validate(model, partition, loss_fn, args):\n",
    "    valloader = DataLoader(partition['val'], \n",
    "                           batch_size=args.batch_size, \n",
    "                           shuffle=False, drop_last=True)\n",
    "    model.eval()\n",
    "\n",
    "    val_acc = 0.0\n",
    "    val_loss = 0.0\n",
    "    with torch.no_grad():\n",
    "        for i, (X, y) in enumerate(valloader):\n",
    "\n",
    "            X = X.transpose(0, 1).float().to(args.device)\n",
    "            y_true = y.float().to(args.device)\n",
    "            model.hidden = [hidden.to(args.device) for hidden in model.init_hidden()]\n",
    "\n",
    "            y_pred = model(X)\n",
    "            loss = loss_fn(y_pred.view(-1), y_true.view(-1))\n",
    "\n",
    "            val_loss += loss.item()\n",
    "            val_acc += metric(y_pred, y_true)\n",
    "\n",
    "    val_loss = val_loss / len(valloader)\n",
    "    val_acc = val_acc / len(valloader)\n",
    "    return val_loss, val_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {
    "id": "kAmbRVca5ZzI"
   },
   "outputs": [],
   "source": [
    "def test(model, partition, args):\n",
    "    testloader = DataLoader(partition['test'], \n",
    "                           batch_size=args.batch_size, \n",
    "                           shuffle=False, drop_last=True)\n",
    "    model.eval()\n",
    "\n",
    "    test_acc = 0.0\n",
    "    cnt = 0\n",
    "    with torch.no_grad():\n",
    "        for i, (X, y) in enumerate(testloader):\n",
    "            cnt += 1\n",
    "#             print(cnt)\n",
    "            X = X.transpose(0, 1).float().to(args.device)\n",
    "            y_true = y.float().to(args.device)\n",
    "            model.hidden = [hidden.to(args.device) for hidden in model.init_hidden()]\n",
    "\n",
    "            y_pred = model(X)\n",
    "            if cnt == 1:\n",
    "                y_true_all = torch.tensor(y_true)\n",
    "                y_pred_all = torch.tensor(y_pred)\n",
    "            else:\n",
    "                y_true_all = torch.cat([y_true_all, y_true], dim=0)\n",
    "                y_pred_all = torch.cat([y_pred_all, y_pred], dim=0)\n",
    "                \n",
    "            test_acc += metric(y_pred, y_true)\n",
    "        \n",
    "    test_acc = test_acc / len(testloader)\n",
    "#     print(len(y_pred_all), len(y_true_all))\n",
    "    \n",
    "    return y_pred, test_acc, y_true_all, y_pred_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {
    "id": "RgQSze2O5ZzJ"
   },
   "outputs": [],
   "source": [
    "def experiment(partition, args):\n",
    "\n",
    "    model = LSTM(args.input_dim, args.hid_dim, args.n_layers, args.batch_size, args.dropout, args.use_bn)\n",
    "    model.to(args.device)\n",
    "\n",
    "    # loss_fn = torch.nn.BCELoss(weight=None, size_average=None, reduce=None, reduction='mean')\n",
    "    loss_fn = nn.BCELoss()\n",
    "    if args.optim == 'SGD':\n",
    "        optimizer = optim.RMSprop(model.parameters(), lr=args.lr, weight_decay=args.l2)\n",
    "    elif args.optim == 'RMSprop':\n",
    "        optimizer = optim.RMSprop(model.parameters(), lr=args.lr, weight_decay=args.l2)\n",
    "    elif args.optim == 'Adam':\n",
    "        optimizer = optim.Adam(model.parameters(), lr=args.lr, weight_decay=args.l2)\n",
    "    else:\n",
    "        raise ValueError('In-valid optimizer choice')\n",
    "    \n",
    "    # ===== List for epoch-wise data ====== #\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "    train_accs = []\n",
    "    val_accs = []\n",
    "    # ===================================== #\n",
    "    \n",
    "    for epoch in range(args.epoch):  # loop over the dataset multiple times\n",
    "        ts = time.time()\n",
    "        model, train_loss, train_acc = train(model, partition, optimizer, loss_fn, args)\n",
    "        val_loss, val_acc = validate(model, partition, loss_fn, args)\n",
    "        te = time.time()\n",
    "        \n",
    "        # ====== Add Epoch Data ====== #\n",
    "        train_losses.append(train_loss)\n",
    "        val_losses.append(val_loss)\n",
    "        train_accs.append(train_acc)\n",
    "        val_accs.append(val_acc)\n",
    "        # ============================ #\n",
    "        \n",
    "        if epoch % 10 == 0:\n",
    "            print('Epoch {}, Acc(train/val): {:2.2f}/{:2.2f}, Loss(train/val) {:2.5f}/{:2.5f}. Took {:2.2f} sec'.format(epoch, train_acc, val_acc, train_loss, val_loss, te-ts))\n",
    "        \n",
    "    y_pred, test_acc, y_true, y_pred_all = test(model, partition, args)   \n",
    "    print('len of temp:', len(temp))\n",
    "    \n",
    "    # ======= Add Result to Dictionary ======= #\n",
    "    result = {}\n",
    "    result['train_losses'] = train_losses\n",
    "    result['val_losses'] = val_losses\n",
    "    result['train_accs'] = train_accs\n",
    "    result['val_accs'] = val_accs\n",
    "    result['train_acc'] = train_acc\n",
    "    result['val_acc'] = val_acc\n",
    "    result['test_acc'] = test_acc\n",
    "#     result['pred'] = y_pred.cpu().detach().numpy()\n",
    "    return vars(args), result, y_true, y_pred_all"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7OUVge945ZzJ"
   },
   "source": [
    "# Manage Experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {
    "id": "6Ved9MOK5ZzJ"
   },
   "outputs": [],
   "source": [
    "import hashlib\n",
    "import json\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "import pandas as pd\n",
    "\n",
    "def save_exp_result(setting, result):\n",
    "    exp_name = setting['exp_name']\n",
    "    del setting['epoch']\n",
    "\n",
    "    hash_key = hashlib.sha1(str(setting).encode()).hexdigest()[:6]\n",
    "    filename = './results/{}-{}.json'.format(exp_name, hash_key)\n",
    "    result.update(setting)\n",
    "    with open(filename, 'w') as f:\n",
    "        json.dump(result, f)\n",
    "\n",
    "    \n",
    "def load_exp_result(exp_name):\n",
    "    dir_path = './results'\n",
    "    filenames = [f for f in listdir(dir_path) if isfile(join(dir_path, f)) if '.json' in f]\n",
    "    list_result = []\n",
    "    for filename in filenames:\n",
    "        if exp_name in filename:\n",
    "            with open(join(dir_path, filename), 'r') as infile:\n",
    "                results = json.load(infile)\n",
    "                list_result.append(results)\n",
    "    df = pd.DataFrame(list_result) # .drop(columns=[])\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {
    "id": "kfLW-ZBo5ZzJ"
   },
   "outputs": [],
   "source": [
    "\n",
    "def plot_acc(var1, var2, df):\n",
    "\n",
    "    fig, ax = plt.subplots(1, 3)\n",
    "    fig.set_size_inches(15, 6)\n",
    "    sns.set_style(\"darkgrid\", {\"axes.facecolor\": \".9\"})\n",
    "\n",
    "    sns.barplot(x=var1, y='train_acc', hue=var2, data=df, ax=ax[0])\n",
    "    sns.barplot(x=var1, y='val_acc', hue=var2, data=df, ax=ax[1])\n",
    "    sns.barplot(x=var1, y='test_acc', hue=var2, data=df, ax=ax[2])\n",
    "    \n",
    "    ax[0].set_title('Train Accuracy')\n",
    "    ax[1].set_title('Validation Accuracy')\n",
    "    ax[2].set_title('Test Accuracy')\n",
    "\n",
    "    \n",
    "def plot_loss_variation(var1, var2, df, **kwargs):\n",
    "\n",
    "    list_v1 = df[var1].unique()\n",
    "    list_v2 = df[var2].unique()\n",
    "    list_data = []\n",
    "\n",
    "    for value1 in list_v1:\n",
    "        for value2 in list_v2:\n",
    "            row = df.loc[df[var1]==value1]\n",
    "            row = row.loc[df[var2]==value2]\n",
    "\n",
    "            train_losses = list(row.train_losses)[0]\n",
    "            val_losses = list(row.val_losses)[0]\n",
    "\n",
    "            for epoch, train_loss in enumerate(train_losses):\n",
    "                list_data.append({'type':'train', 'loss':train_loss, 'epoch':epoch, var1:value1, var2:value2})\n",
    "            for epoch, val_loss in enumerate(val_losses):\n",
    "                list_data.append({'type':'val', 'loss':val_loss, 'epoch':epoch, var1:value1, var2:value2})\n",
    "\n",
    "    df = pd.DataFrame(list_data)\n",
    "    g = sns.FacetGrid(df, row=var2, col=var1, hue='type', **kwargs)\n",
    "    g = g.map(plt.plot, 'epoch', 'loss', marker='.')\n",
    "    g.add_legend()\n",
    "    g.fig.suptitle('Train loss vs Val loss')\n",
    "    plt.subplots_adjust(top=0.89) \n",
    "\n",
    "\n",
    "def plot_acc_variation(var1, var2, df, **kwargs):\n",
    "    list_v1 = df[var1].unique()\n",
    "    list_v2 = df[var2].unique()\n",
    "    list_data = []\n",
    "\n",
    "    for value1 in list_v1:\n",
    "        for value2 in list_v2:\n",
    "            row = df.loc[df[var1]==value1]\n",
    "            row = row.loc[df[var2]==value2]\n",
    "\n",
    "            train_accs = list(row.train_accs)[0]\n",
    "            val_accs = list(row.val_accs)[0]\n",
    "            test_acc = list(row.test_acc)[0]\n",
    "\n",
    "            for epoch, train_acc in enumerate(train_accs):\n",
    "                list_data.append({'type':'train', 'Acc':train_acc, 'test_acc':test_acc, 'epoch':epoch, var1:value1, var2:value2})\n",
    "            for epoch, val_acc in enumerate(val_accs):\n",
    "                list_data.append({'type':'val', 'Acc':val_acc, 'test_acc':test_acc, 'epoch':epoch, var1:value1, var2:value2})\n",
    "\n",
    "    df = pd.DataFrame(list_data)\n",
    "    g = sns.FacetGrid(df, row=var2, col=var1, hue='type', **kwargs)\n",
    "    g = g.map(plt.plot, 'epoch', 'Acc', marker='.')\n",
    "\n",
    "    def show_acc(x, y, metric, **kwargs):\n",
    "        plt.scatter(x, y, alpha=0.3, s=1)\n",
    "        metric = \"Test Acc: {:1.3f}\".format(list(metric.values)[0])\n",
    "        plt.text(0.05, 0.95, metric,  horizontalalignment='left', verticalalignment='center', transform=plt.gca().transAxes, bbox=dict(facecolor='yellow', alpha=0.5, boxstyle=\"round,pad=0.1\"))\n",
    "    g = g.map(show_acc, 'epoch', 'Acc', 'test_acc')\n",
    "\n",
    "    g.add_legend()\n",
    "    g.fig.suptitle('Train Accuracy vs Val Accuracy')\n",
    "    plt.subplots_adjust(top=0.89)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10706.0\n"
     ]
    }
   ],
   "source": [
    "train_val_dataset = total[:int((len(total) / 4)*.8)*4]\n",
    "test_dataset = total[int((len(total) / 4)*.8)*4:]\n",
    "train_dataset = train_val_dataset[:int((len(train_val_dataset) / 4)*.8)*4]\n",
    "valid_dataset = train_val_dataset[int((len(train_val_dataset) / 4)*.8)*4:]\n",
    "\n",
    "trainset = SepsisDataset(train_dataset, seq_len=4)\n",
    "valset = SepsisDataset(valid_dataset, seq_len=4)\n",
    "testset = SepsisDataset(test_dataset, seq_len=4)\n",
    "\n",
    "print(len(test_dataset)/4)\n",
    "\n",
    "partition = {'train': trainset, 'val':valset, 'test':testset}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {
    "id": "pqir1JP_5ZzH"
   },
   "outputs": [],
   "source": [
    "def metric(y_pred, y_true):  \n",
    "#     auc = roc_auc_score(y_true, y_pred) \n",
    "    \n",
    "    perc_y_pred = y_pred.cpu().detach().numpy()\n",
    "    perc_y_true = y_true.cpu().detach().numpy()\n",
    "\n",
    "    perc_y_pred = np.where(perc_y_pred > 0.5, 1, 0)\n",
    "    perc_y_pred = np.squeeze(perc_y_pred)\n",
    "    \n",
    "#     auc = accuracy_score(perc_y_true, perc_y_pred, normalize=True)\n",
    "    \n",
    "#     print('pred:', perc_y_pred)\n",
    "#     print('true:', perc_y_true)\n",
    "#     auc = roc_auc_score(perc_y_true, perc_y_pred)\n",
    "    try:\n",
    "        auc = roc_auc_score(perc_y_true, perc_y_pred)\n",
    "    except ValueError:\n",
    "        print('Error')\n",
    "        auc = 0.00\n",
    "\n",
    "    return auc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 375
    },
    "id": "D2umJYQT5ZzK",
    "outputId": "96e326e4-74a7-4428-b5f5-213e83ae931c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Namespace(batch_size=64, device='cuda', dropout=0.0, epoch=50, exp_name='exp1_lr', hid_dim=60, input_dim=3, l2=1e-05, lr=0.0001, n_layers=3, optim='Adam', use_bn=True)\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Epoch 0, Acc(train/val): 0.49/0.50, Loss(train/val) 0.40360/0.25947. Took 3.84 sec\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-237-1c3b2dc71921>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     40\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 42\u001b[0;31m         \u001b[0msetting\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred_all\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mexperiment\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpartition\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdeepcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     43\u001b[0m         \u001b[0msave_exp_result\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msetting\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-207-1554c44e0295>\u001b[0m in \u001b[0;36mexperiment\u001b[0;34m(partition, args)\u001b[0m\n\u001b[1;32m     24\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# loop over the dataset multiple times\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m         \u001b[0mts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m         \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_acc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpartition\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m         \u001b[0mval_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_acc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalidate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpartition\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m         \u001b[0mte\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-204-fa925aed5a49>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(model, partition, optimizer, loss_fn, args)\u001b[0m\n\u001b[1;32m     26\u001b[0m         \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_pred\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_true\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    253\u001b[0m                 \u001b[0mcreate_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    254\u001b[0m                 inputs=inputs)\n\u001b[0;32m--> 255\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    256\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    257\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    147\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m    148\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 149\u001b[0;31m         allow_unreachable=True, accumulate_grad=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m    150\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    151\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# ====== Random Seed Initialization ====== #\n",
    "seed = 666\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "\n",
    "parser = argparse.ArgumentParser()\n",
    "args = parser.parse_args(\"\")\n",
    "args.exp_name = \"exp1_lr\"\n",
    "args.device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "args.batch_size = 64\n",
    "\n",
    "# ====== Model Capacity ===== #\n",
    "args.input_dim = 3\n",
    "args.hid_dim = 60\n",
    "args.n_layers = 2\n",
    "\n",
    "# ====== Regularization ======= #\n",
    "args.l2 = 0.00001\n",
    "args.dropout = 0.0\n",
    "args.use_bn = True\n",
    "\n",
    "# ====== Optimizer & Training ====== #\n",
    "args.optim = 'Adam' #'RMSprop' #SGD, RMSprop, ADAM...\n",
    "args.lr = 0.0001\n",
    "args.epoch = 50\n",
    "\n",
    "\n",
    "# ====== Experiment Variable ====== #\n",
    "name_var1 = 'lr'\n",
    "name_var2 = 'n_layers'\n",
    "# list_var1 = [0.001, 0.0001, 0.00001]\n",
    "list_var1 = [0.0001]\n",
    "# list_var2 = [1,2,3]\n",
    "list_var2 = [3]\n",
    "\n",
    "for var1 in list_var1:\n",
    "    for var2 in list_var2:\n",
    "        setattr(args, name_var1, var1)\n",
    "        setattr(args, name_var2, var2)\n",
    "        print(args)\n",
    "                \n",
    "        setting, result, y_true, y_pred_all = experiment(partition, deepcopy(args))\n",
    "        save_exp_result(setting, result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "N-lGOJcT5ZzK"
   },
   "outputs": [],
   "source": [
    "var1 = 'lr'\n",
    "var2 = 'n_layers'\n",
    "df = load_exp_result('exp1')\n",
    "\n",
    "plot_acc(var1, var2, df)\n",
    "plot_loss_variation(var1, var2, df, sharey=False) #sharey를 True로 하면 모둔 subplot의 y축의 스케일이 같아집니다.\n",
    "plot_acc_variation(var1, var2, df, margin_titles=True, sharey=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_all = y_pred_all.cpu().detach().numpy()\n",
    "y_true = y_true.cpu().detach().numpy()\n",
    "y_pred_all = np.where(y_pred_all > 0.5, 1, 0)\n",
    "\n",
    "print(len(y_pred_all))\n",
    "print(len(y_true))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc = accuracy_score(y_true, y_pred_all, normalize=True)\n",
    "auc = roc_auc_score(y_true, y_pred_all)\n",
    "auc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "t = classification_report(y_true, y_pred_all, target_names=['True', 'False'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 혼동행렬, 정확도, 정밀도, 재현율, F1, AUC 불러오기\n",
    "def get_clf_eval(y_test, y_pred):\n",
    "    confusion = confusion_matrix(y_test, y_pred)\n",
    "    tt = confusion[0,0]\n",
    "    tf = confusion[0,1]\n",
    "    ft = confusion[1,0]\n",
    "    ff = confusion[1,1]\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    precision = precision_score(y_test, y_pred,average='weighted') # None, 'micro', 'macro', 'weighted'\n",
    "    recall = recall_score(y_test, y_pred,average='weighted')\n",
    "    F1 = f1_score(y_test, y_pred,average='weighted')\n",
    "    print('오차행렬:\\n', confusion)\n",
    "    print('\\n정확도: {:.4f}'.format(accuracy))\n",
    "    print('정밀도: {:.4f}'.format(precision))\n",
    "    print('재현율: {:.4f}'.format(recall))    \n",
    "    print('양성예측율: {:.4f}'.format(tt/(tt+tf)))\n",
    "    print('음성예측율: {:.4f}'.format(ff/(ft+ff)))    \n",
    "    print('F1: {:.4f}'.format(F1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_clf_eval(y_true, y_pred_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "auc"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Sepsis_with_LSTM.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
