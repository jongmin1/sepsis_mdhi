{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "colab": {
      "name": "LSTM.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gqlpt6bO5ZzD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "04886b08-66a6-4076-c8a9-7d78da19622b"
      },
      "source": [
        "import pandas as pd\n",
        "import datetime\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import torch.optim as optim\n",
        "import numpy as np\n",
        "import argparse\n",
        "from copy import deepcopy # Add Deepcopy for args\n",
        "from sklearn.metrics import mean_absolute_error\n",
        "\n",
        "import seaborn as sns \n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "print(torch.__version__)\n",
        "%matplotlib inline\n",
        "%pylab inline\n",
        "pylab.rcParams['figure.figsize'] = (15, 9)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1.9.0+cu102\n",
            "Populating the interactive namespace from numpy and matplotlib\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xTdQQjnK5ZzF"
      },
      "source": [
        "# Data Preparation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VhaAjBEQ5ZzG"
      },
      "source": [
        "class PatientDataset(Dataset):  # i번째 record를 주는 것\n",
        "    \n",
        "    def __init__(self, x_frames, y_frames):\n",
        "        # x_frame: 최근 n 만큼\n",
        "        # y_frame: 이후 n 만큼\n",
        "        self.x_frames = x_frames\n",
        "        self.y_frames = y_frames\n",
        "        \n",
        "        # load dataset in pd\n",
        "        \n",
        "        print(self.data.isna().sum())\n",
        "        \n",
        "    def __len__(self):  \n",
        "      # 있는 data(1,2,3,4,5)에서 최근 2, 이후 2 주면\n",
        "      # 1,2-3,4 / 2,3-4,5 이렇게 만들 수 있는 sequence는 2개이므로 이걸 수식으로 표현하면 아래\n",
        "        return len(self.data) - (self.x_frames + self.y_frames) + 1\n",
        "    \n",
        "    def __getitem__(self, idx):\n",
        "        idx += self.x_frames\n",
        "        data = self.data.iloc[idx-self.x_frames:idx+self.y_frames]\n",
        "        data = data[['Ox', 'CO2', 'BPb', 'BPs', 'HR', 'PH', 'Resira', 'Temper', 'WBC']]\n",
        "        data = data.apply(lambda x: np.log(x+1) - np.log(x[self.x_frames-1]+1)) # 일종의 normalization 이거 앞에서 한번에 해줄까\n",
        "        data = data.values # to numpy array\n",
        "        X = data[:self.x_frames]  # 뽑은 데이터에서 최근 n개\n",
        "        y = data[self.x_frames:]  # 뽑은 데이터에서  이후 m개\n",
        "        \n",
        "        return X, y"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Al40scAi5ZzG"
      },
      "source": [
        "# Model Define"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Rr7F5kpl5ZzH"
      },
      "source": [
        "class LSTM(nn.Module):\n",
        "    \n",
        "    def __init__(self, input_dim, hidden_dim, output_dim, num_layers, batch_size, dropout, use_bn):\n",
        "        super(LSTM, self).__init__()\n",
        "        self.input_dim = input_dim \n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.output_dim = output_dim\n",
        "        self.num_layers = num_layers\n",
        "\n",
        "        self.batch_size = batch_size\n",
        "        self.dropout = dropout\n",
        "        self.use_bn = use_bn \n",
        "        \n",
        "        self.lstm = nn.LSTM(self.input_dim, self.hidden_dim, self.num_layers)\n",
        "        self.hidden = self.init_hidden()\n",
        "        self.regressor = self.make_regressor()\n",
        "        \n",
        "    def init_hidden(self):\n",
        "        return (torch.zeros(self.num_layers, self.batch_size, self.hidden_dim),\n",
        "                torch.zeros(self.num_layers, self.batch_size, self.hidden_dim))\n",
        "    \n",
        "    def make_regressor(self):\n",
        "        layers = []\n",
        "        if self.use_bn:\n",
        "            layers.append(nn.BatchNorm1d(self.hidden_dim))\n",
        "        layers.append(nn.Dropout(self.dropout))\n",
        "        \n",
        "        # 나중에 network(node 개수 바꿔 주는 옵션도 생각하기)\n",
        "        layers.append(nn.Linear(self.hidden_dim, self.hidden_dim // 2))\n",
        "        layers.append(nn.ReLU())\n",
        "        layers.append(nn.Linear(self.hidden_dim // 2, self.output_dim))\n",
        "        regressor = nn.Sequential(*layers)\n",
        "        return regressor\n",
        "    \n",
        "    def forward(self, x):\n",
        "      # Model Capacity 따라 y.sahpe이 달라짐\n",
        "        lstm_out, self.hidden = self.lstm(x, self.hidden)\n",
        "        y_pred = self.regressor(lstm_out[-1].view(self.batch_size, -1)) # 맨 마지막 h_t 갖고 예측 할거니까 -1\n",
        "        return y_pred"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pqir1JP_5ZzH"
      },
      "source": [
        "def metric(y_pred, y_true):  \n",
        "    auc = roc_auc_score(y_true, y_pred)\n",
        "    return auc"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0j20Sve95ZzH"
      },
      "source": [
        "# Train, Validate, Test "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I7FqJ9EX5ZzI"
      },
      "source": [
        "def train(model, partition, optimizer, loss_fn, args):\n",
        "  # batch 단위로 갖고 오는 것\n",
        "  # 무작위로 시작 record 갖고 옴\n",
        "  # 갖고 올때 batch size만큼 갖고 옴\n",
        "  # permutation, not random\n",
        "    trainloader = DataLoader(partition['train'], \n",
        "                             batch_size=args.batch_size,\n",
        "                             shuffle=True, drop_last=True)\n",
        "    model.train()\n",
        "    model.zero_grad()\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    train_acc = 0.0\n",
        "    train_loss = 0.0\n",
        "    for i, (X, y) in enumerate(trainloader):\n",
        "        # batch size, 12달치가 있는데, 이게 \n",
        "        # X : [10, n, 6] [n, 10, 6]->[time_series, batch size, input dim] \n",
        "        # Y : [10, m, 1]  \n",
        "        X = X.transpose(0, 1).float().to(args.device)\n",
        "        # 3번째 column에 대해서만 결과값을 얻기 위해서\n",
        "        # y[:, :, :] 하면 다른 거까지 다 나오는데 \n",
        "        # 라벨링해서 하나만 뽑을지, 아니면 다 뽑고 계산하는게 나을지\n",
        "        # 바꿀 때 validate, test도 바꿔주기\n",
        "        y_true = y.float().to(args.device) \n",
        "        #print(torch.max(X[:, :, 3]), torch.max(y_true))\n",
        "\n",
        "        model.zero_grad()\n",
        "        optimizer.zero_grad()\n",
        "        model.hidden = [hidden.to(args.device) for hidden in model.init_hidden()]\n",
        "\n",
        "        y_pred = model(X)\n",
        "        loss = loss_fn(y_pred.view(-1), y_true.view(-1)). # 마지막 h_t\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        train_loss += loss.item()\n",
        "        train_acc += metric(y_pred, y_true)[0]\n",
        "\n",
        "    train_loss = train_loss / len(trainloader)\n",
        "    train_acc = train_acc / len(trainloader)\n",
        "    return model, train_loss, train_acc"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JTX3hfCI5ZzI"
      },
      "source": [
        "def validate(model, partition, loss_fn, args):\n",
        "    valloader = DataLoader(partition['val'], \n",
        "                           batch_size=args.batch_size, \n",
        "                           shuffle=False, drop_last=True)\n",
        "    model.eval()\n",
        "\n",
        "    val_acc = 0.0\n",
        "    val_loss = 0.0\n",
        "    with torch.no_grad():\n",
        "        for i, (X, y) in enumerate(valloader):\n",
        "\n",
        "            X = X.transpose(0, 1).float().to(args.device)\n",
        "            y_true = y.float().to(args.device)\n",
        "            model.hidden = [hidden.to(args.device) for hidden in model.init_hidden()]\n",
        "\n",
        "            y_pred = model(X)\n",
        "            loss = loss_fn(y_pred.view(-1), y_true.view(-1))\n",
        "\n",
        "            val_loss += loss.item()\n",
        "            val_acc += metric(y_pred, y_true)[0]\n",
        "\n",
        "    val_loss = val_loss / len(valloader)\n",
        "    val_acc = val_acc / len(valloader)\n",
        "    return val_loss, val_acc"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kAmbRVca5ZzI"
      },
      "source": [
        "def test(model, partition, args):\n",
        "    testloader = DataLoader(partition['test'], \n",
        "                           batch_size=args.batch_size, \n",
        "                           shuffle=False, drop_last=True)\n",
        "    model.eval()\n",
        "\n",
        "    test_acc = 0.0\n",
        "    with torch.no_grad():\n",
        "        for i, (X, y) in enumerate(testloader):\n",
        "\n",
        "            X = X.transpose(0, 1).float().to(args.device)\n",
        "            y_true = y.float().to(args.device)\n",
        "            model.hidden = [hidden.to(args.device) for hidden in model.init_hidden()]\n",
        "\n",
        "            y_pred = model(X)\n",
        "            test_acc += metric(y_pred, y_true)[0]\n",
        "\n",
        "    test_acc = test_acc / len(testloader)\n",
        "    return test_acc"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RgQSze2O5ZzJ"
      },
      "source": [
        "def experiment(partition, args):\n",
        "\n",
        "    model = LSTM(args.input_dim, args.hid_dim, args.y_frames, args.n_layers, args.batch_size, args.dropout, args.use_bn)\n",
        "    model.to(args.device)\n",
        "    loss_fn = torch.nn.MSELoss()\n",
        "\n",
        "    # loss_fn = torch.nn.BCELoss(weight=None, size_average=None, reduce=None, reduction='mean')\n",
        "    loss_fn = nn.MSELoss()\n",
        "    if args.optim == 'SGD':\n",
        "        optimizer = optim.RMSprop(model.parameters(), lr=args.lr, weight_decay=args.l2)\n",
        "    elif args.optim == 'RMSprop':\n",
        "        optimizer = optim.RMSprop(model.parameters(), lr=args.lr, weight_decay=args.l2)\n",
        "    elif args.optim == 'Adam':\n",
        "        optimizer = optim.Adam(model.parameters(), lr=args.lr, weight_decay=args.l2)\n",
        "    else:\n",
        "        raise ValueError('In-valid optimizer choice')\n",
        "    \n",
        "    # ===== List for epoch-wise data ====== #\n",
        "    train_losses = []\n",
        "    val_losses = []\n",
        "    train_accs = []\n",
        "    val_accs = []\n",
        "    # ===================================== #\n",
        "        \n",
        "    for epoch in range(args.epoch):  # loop over the dataset multiple times\n",
        "        ts = time.time()\n",
        "        model, train_loss, train_acc = train(model, partition, optimizer, loss_fn, args)\n",
        "        val_loss, val_acc = validate(model, partition, loss_fn, args)\n",
        "        te = time.time()\n",
        "        \n",
        "        # ====== Add Epoch Data ====== #\n",
        "        train_losses.append(train_loss)\n",
        "        val_losses.append(val_loss)\n",
        "        train_accs.append(train_acc)\n",
        "        val_accs.append(val_acc)\n",
        "        # ============================ #\n",
        "        \n",
        "        print('Epoch {}, Acc(train/val): {:2.2f}/{:2.2f}, Loss(train/val) {:2.5f}/{:2.5f}. Took {:2.2f} sec'.format(epoch, train_acc, val_acc, train_loss, val_loss, te-ts))\n",
        "        \n",
        "    test_acc = test(model, partition, args)    \n",
        "    \n",
        "    # ======= Add Result to Dictionary ======= #\n",
        "    result = {}\n",
        "    result['train_losses'] = train_losses\n",
        "    result['val_losses'] = val_losses\n",
        "    result['train_accs'] = train_accs\n",
        "    result['val_accs'] = val_accs\n",
        "    result['train_acc'] = train_acc\n",
        "    result['val_acc'] = val_acc\n",
        "    result['test_acc'] = test_acc\n",
        "    return vars(args), result"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7OUVge945ZzJ"
      },
      "source": [
        "# Manage Experiment"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2tafokGd5ZzJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2158a1d4-a6e7-4e53-cff4-0ee8024971b9"
      },
      "source": [
        "\n",
        "trainset = StockDataset(args.x_frames, args.y_frames, 'train')\n",
        "valset = StockDataset(args.x_frames, args.y_frames, 'valid')\n",
        "testset = StockDataset(args.x_frames, args.y_frames, 'test')\n",
        "partition = {'train': trainset, 'val':valset, 'test':testset}\n",
        "print(partition['train'])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[*********************100%***********************]  1 of 1 completed\n",
            "Open         0\n",
            "High         0\n",
            "Low          0\n",
            "Close        0\n",
            "Adj Close    0\n",
            "Volume       0\n",
            "dtype: int64\n",
            "[*********************100%***********************]  1 of 1 completed\n",
            "Open         0\n",
            "High         0\n",
            "Low          0\n",
            "Close        0\n",
            "Adj Close    0\n",
            "Volume       0\n",
            "dtype: int64\n",
            "[*********************100%***********************]  1 of 1 completed\n",
            "Open         0\n",
            "High         0\n",
            "Low          0\n",
            "Close        0\n",
            "Adj Close    0\n",
            "Volume       0\n",
            "dtype: int64\n",
            "<__main__.StockDataset object at 0x7ff140f7ab50>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D2umJYQT5ZzK",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 375
        },
        "outputId": "96e326e4-74a7-4428-b5f5-213e83ae931c"
      },
      "source": [
        "# ====== Random Seed Initialization ====== #\n",
        "seed = 666\n",
        "np.random.seed(seed)\n",
        "torch.manual_seed(seed)\n",
        "\n",
        "parser = argparse.ArgumentParser()\n",
        "args = parser.parse_args(\"\")\n",
        "args.exp_name = \"exp1_lr\"\n",
        "args.device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "# ====== Data Loading ====== #\n",
        "args.batch_size = 128\n",
        "args.x_frames = 5\n",
        "args.y_frames = 5\n",
        "\n",
        "# ====== Model Capacity ===== #\n",
        "args.input_dim = 6\n",
        "args.hid_dim = 50\n",
        "args.n_layers = 2\n",
        "\n",
        "# ====== Regularization ======= #\n",
        "args.l2 = 0.00001\n",
        "args.dropout = 0.0\n",
        "args.use_bn = True\n",
        "\n",
        "# ====== Optimizer & Training ====== #\n",
        "args.optim = 'RMSprop' #'RMSprop' #SGD, RMSprop, ADAM...\n",
        "args.lr = 0.0001\n",
        "args.epoch = 2\n",
        "\n",
        "\n",
        "# ====== Experiment Variable ====== #\n",
        "name_var1 = 'lr'\n",
        "name_var2 = 'n_layers'\n",
        "list_var1 = [0.001, 0.0001, 0.00001]\n",
        "list_var2 = [1,2,3]\n",
        "\n",
        "\n",
        "for var1 in list_var1:\n",
        "    for var2 in list_var2:\n",
        "        setattr(args, name_var1, var1)\n",
        "        setattr(args, name_var2, var2)\n",
        "        print(args)\n",
        "                \n",
        "        setting, result = experiment(partition, deepcopy(args))\n",
        "        # save_exp_result(setting, result)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Namespace(batch_size=128, device='cuda', dropout=0.0, epoch=2, exp_name='exp1_lr', hid_dim=50, input_dim=6, l2=1e-05, lr=0.001, n_layers=1, optim='RMSprop', symbol='AAPL', use_bn=True, x_frames=5, y_frames=5)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-16-129e58ad3af5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     44\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 46\u001b[0;31m         \u001b[0msetting\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mexperiment\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpartition\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdeepcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     47\u001b[0m         \u001b[0;31m# save_exp_result(setting, result)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-12-2d11fb34fa6f>\u001b[0m in \u001b[0;36mexperiment\u001b[0;34m(partition, args)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mexperiment\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpartition\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mLSTM\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minput_dim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhid_dim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0my_frames\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_layers\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdropout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muse_bn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;31m# loss_fn = torch.nn.MSELoss()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-7-947e8ab15785>\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, input_dim, hidden_dim, output_dim, num_layers, batch_size, dropout, use_bn)\u001b[0m\n\u001b[1;32m     14\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlstm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLSTM\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minput_dim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhidden_dim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_layers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhidden\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minit_hidden\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mregressor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmake_regressor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0minit_hidden\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-7-947e8ab15785>\u001b[0m in \u001b[0;36mmake_regressor\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     30\u001b[0m         \u001b[0mlayers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mReLU\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m         \u001b[0mlayers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhidden_dim\u001b[0m \u001b[0;34m//\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput_dim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 32\u001b[0;31m         \u001b[0mlayers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msigmoid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     33\u001b[0m         \u001b[0mregressor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSequential\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mregressor\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: module 'torch.nn' has no attribute 'sigmoid'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pKevCSOB5ZzK"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}